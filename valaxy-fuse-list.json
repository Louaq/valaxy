[{"title":"YOLOv8的训练自己的数据集","tags":["python","Deep Learning"],"categories":["深度学习"],"author":"Louaq","excerpt":"\n一、YOLOv8的简介\n-----------\n\nYOLO（You Only Look Once）系列算法因其高效、准确等特点而备受瞩目。由2023年Ultralytics公司发布了YOLO的**最","link":"/posts/article_1","content":"\n一、YOLOv8的简介\n-----------\n\nYOLO（You Only Look Once）系列算法因其高效、准确等特点而备受瞩目。由2023年Ultralytics公司发布了YOLO的**最新版本YOLOv8是结合前几代YOLO的基础上的一个融合改进版**。\n\n本文YOLOv8网络结构/环境搭建/数据集获取/训练/推理/验证/导出/部署，从网络结构的讲解从模型的网络结构讲解到模型的部署都有详细介绍，同时在本专栏中还包括YOLOv8模型系列的改进包**括个人提出的创新点，传统卷积、注意力机制、损失函数的修改教程，能够帮助你的论文获得创新点。**\n\n\n\n二、YOLOv8相对于Yolov5的核心改动\n----------------------\n\n![39fa749365bc4a6e87a8e63563bca5cc.png](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/39fa749365bc4a6e87a8e63563bca5cc.png)\n\n* * *\n\n从YOLOv8的网络结构可以看出,其延用了YOLOv5的网络结构思想，**包括基于CSP（紧凑和分离）的骨干网络(backbone)和Neck部分的设计，以及对于不同尺度模型的考虑。**\n\n**改进总结：**\n\n> 1.  Backbone的改进：使用C2f模块代替C3模块，进一步轻量化，同时保持了CSP的思想，同时采用了SPPF模块。\n>     \n> 2.  PAN-FPN的改进：保留了PAN的思想，但删除了上采样阶段中的卷积结构，同时将C3模块替换为C2f模块。\n>     \n> 3.  Decoupled-Head的引入：采用了Decoupled-Head的思想，使得网络的训练和推理更加高效。\n>     \n> 4.  Anchor-Free的思想：抛弃了Anchor-Base，采用了Anchor-Free的思想。\n>     \n> 5.  损失函数的改进：采用VFL Loss作为分类损失，同时使用DFL Loss和CIOU Loss作为回归损失。\n>     \n> 6.  样本匹配方式的改进：采用了Task-Aligned Assigner匹配方式。\n>     \n\n这些改进使得YOLOv8在目标检测方面具有更高的精度和更快的速度，同时保持了轻量化的特点。\n\n**具体来说**，YOLOv8的Backbone部分使用了C2f模块来替代了YOLOv5中的C3模块，实现了进一步的轻量化。同时，它也保留了YOLOv5等架构中使用的SPPF（空间金字塔池化）模块。\n\n在PAN-FPN（路径聚合网络-特征金字塔网络）部分，虽然YOLOv8依旧采用了PAN的思想，但是在结构上，它删除了YOLOv5中PAN-FPN上采样阶段中的卷积结构，并将C3模块替换为了C2f模块。\n\n这些改进使得YOLOv8在保持了YOLOv5网络结构的优点的同时，进行了更加精细的调整和优化，提高了模型在不同场景下的性能。\n\n**三、YOLOv8的网络结构**\n-----------------\n\nYOLOv8的网络结构主要由以下三个大部分组成：\n\n> 1.  Backbone：它采用了一系列卷积和反卷积层来提取特征，同时也使用了残差连接和瓶颈结构来减小网络的大小和提高性能。该部分采用了C2f模块作为基本构成单元，与YOLOv5的C3模块相比，C2f模块具有更少的参数量和更优秀的特征提取能力。\n>     \n> 2.  Neck：它采用了多尺度特征融合技术，将来自Backbone的不同阶段的特征图进行融合，以增强特征表示能力。具体来说，YOLOv8的Neck部分包括一个SPPF模块、一个PAA模块和两个PAN模块。\n>     \n> 3.  Head：它负责最终的目标检测和分类任务，包括一个检测头和一个分类头。检测头包含一系列卷积层和反卷积层，用于生成检测结果；分类头则采用全局平均池化来对每个特征图进行分类。\n>     \n\n下面我们来针对于YOLOv8的三个组成部分进行详细讲解。\n\n#### 3.1 Backbone\n\n由最上面的YOLOv8网络结构图我们可以看出在其中的Backbone部分，由5个卷积模块和4个C2f模块和一个SPPF模块组成，\n\n![a744491cd5a14bcf9b33015b18c6c6c8.png](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a744491cd5a14bcf9b33015b18c6c6c8.png)\n\n(其中浅蓝色为卷积模块,黄色为C2f模块深蓝色为SPPF模块 )\n\n如果上图看的不够直观,我们来看一下YOLOv8的文件中的yaml文件,看一下它backbone部分的结构组成部分，会更加直观。 \n\n```python\nbackbone:\n  # [from, repeats, module, args]\n  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2\n  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4\n  - [-1, 3, C2f, [128, True]]\n  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8\n  - [-1, 6, C2f, [256, True]]\n  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16\n  - [-1, 6, C2f, [512, True]]\n  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32\n  - [-1, 3, C2f, [1024, True]]\n  - [-1, 1, SPPF, [1024, 5]]  # 9\n```\n\n上面的部分就是YOLOv8的yaml文件的Backbone部分，可以看到其由5个Conv模块，四个C2f模块以及一个SPPF模块组成，**下面我们来根据每个模块的组成来进行讲解。**\n\n##### 3.1.1 卷积模块(Conv)\n\n在其中卷积模块的结构主要为下图\n\n![56694f12be0d4664905561c9438e2850.png](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/56694f12be0d4664905561c9438e2850.png)\n\n在其中主要结构为一个2D的卷积一个BatchNorm2d和一个SiLU激活函数，整个**卷积模块**的作用为：\n\n1.  **降采样：每个卷积模块中的卷积层都采用步长为2的卷积核进行降采样操作，以减小特征图的尺寸并增加通道数。**\n2.  **非线性表示：每个卷积层之后都添加了Batch Normalization（批标准化）层和ReLU激活函数，以增强模型的非线性表示能力。**\n\n> 在其中Batch Normalization（批标准化）是深度学习中常用的一种技术，用于加速神经网络的训练。Batch Normalization通过对每个小批量数据进行标准化，使得神经网络在训练过程中更加稳定，可以使用更高的学习率，并且减少了对初始化权重的依赖。Batch Normalization的基本思想是：对每个小批量数据进行标准化，使得每个特征的均值为0，方差为1，然后再通过一个可学习的缩放因子和平移因子来调整数据的分布，从而使得神经网络更容易训练。\n\n##### **3.1.2 C2f模块**\n\n在YOLOv8的网络结构中C2f模块算是YOLOv8的一个较大的改变，与YOLOv5的C3模块相比，C2f模块具有更少的参数量和更优秀的特征提取能力。**下图为C2f的内部网络结构图。**\n\n![3fa4855f3d38447b93e5faf40cd59169.png](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/3fa4855f3d38447b93e5faf40cd59169.png)\n\n在C2f模块中我们可以看到输入首先经过**一个k=1，s=1，p=0，c=c\\_out**的卷积模块进行了处理，然后经过一个split处理**(在这里split和后面的concat的组成其实就是所谓的残差模块处理)**经过数量为n的DarknetBottleneck模块处理以后将残差模块和主干模块的结果进行Concat拼接在经过一个卷积模块处理进行输出。 \n\n> 在其中提到的残差连接（residual connections）是一种用于构建深层神经网络的技术。它的核心思想是通过跳过层级连接来传递残差或误差。\n> \n> 在传统的神经网络中，信息流通过一层层的网络层，每一层都通过非线性激活函数进行转换和提取特征。然而，随着神经网络的加深，可能会出现\"梯度消失\"或\"梯度爆炸\"的问题，导致网络收敛困难或性能下降。\n> \n> 残差连接通过引入跨层级的连接，将输入的原始信息直接传递到后续层级，以解决梯度消失和爆炸问题。具体而言，它将网络的输入与中间层的输出相加，形成了一个\"捷径\"或\"跳跃连接\"，从而允许梯度更容易地传播。\n> \n> 数学上，假设我们有一个输入x，通过多个网络层进行处理后得到预测值H(x)。那么残差连接的表达式为：\n> \n> F(x) = H(x) + x\n> \n> 其中，F(x)为残差块的输出，H(x)为经过一系列网络层处理后得到的特征表示，x为输入直接连接到残差块中的跳跃连接。\n> \n> 通过残差连接，网络可以更容易地学习残差或误差，从而使网络更深层次的特征表达更准确。这对于训练深层神经网络非常有用，可以提高网络的性能和收敛速度。\n\n>  在C2f模块中用到的DarknetBottleneck模块其中使用多个3x3卷积核进行卷积操作，提取特征信息。同时其具有add是否进行残差链接的选项。\n\n **其实整个C2f模块就是一个改良版本的Darknet**\n\n1.  首先，使用1x1卷积核将输入通道数减少到原来的1/2，以减少计算量和内存消耗。\n    \n2.  然后，使用多个3x3卷积核进行卷积操作，提取特征信息。\n    \n3.  接着，使用残差链接，将输入直接加到输出中，从而形成了一条跨层连接。\n    \n4.  接着，再次使用1x1卷积核恢复特征图的通道数。\n\nSPPF模块 \n\nYOLOv8的SPPF模块相对于YOLOv5的SPPF模块并没有任何的改变。\n\n#### 3.2 Neck \n\nYOLOv8的Neck部分是该模型中的一个关键组件，**它在特征提取和融合方面起着重要作用**。Neck的详细描述如下：\n\nNeck部分主要起到一个特征融合的操作, YOLOv8的Neck部分依然采用PAN-FPN的思想，下图的a，b，c为一个Neck部分的流程示意图。\n\n![a3aeff6d8f0542a79efbd5e95c0b10b9.png](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a3aeff6d8f0542a79efbd5e95c0b10b9.png)\n\n> 整个Neck部分的步骤如下：：将特征提取网络(Backbone)的输出P3，P4，P5输入进PAN-FPN网络结构，使得多个尺度的特征图进行融合；将P5经过上采样与P4进行融合得到F1，将F1经过C2f层和一次上采样与P3进行融合得到T1，将T1经过一次卷积层与F1经过融合得到F2，将F2经过一次C2f层得到T2，将T2经过一次卷积层与P5融合得到F3，将F3经过一次C2f层得到T3，最终得到T1、T2、T3就是整个Neck的产物； \n\n上述过程可以描述为下图，我在图片上做了一些标准方便理解。\n\n![4b480765acd947879588f6d132704eb8.jpeg](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/4b480765acd947879588f6d132704eb8.jpeg)\n\n上述的过程可以在代码部分看到,我们同样看YOLOv8的yaml文件，能够更直观的看到这个步骤,大家可以看代码同时对应图片来进行分析:\n\n```python\nhead:\n  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4\n  - [-1, 3, C2f, [512]]  # 12\n \n  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3\n  - [-1, 3, C2f, [256]]  # 15 (P3/8-small)\n \n  - [-1, 1, Conv, [256, 3, 2]]\n  - [[-1, 12], 1, Concat, [1]]  # cat head P4\n  - [-1, 3, C2f, [512]]  # 18 (P4/16-medium)\n \n  - [-1, 1, Conv, [512, 3, 2]]\n  - [[-1, 9], 1, Concat, [1]]  # cat head P5\n  - [-1, 3, C2f, [1024]]  # 21 (P5/32-large)\n```\n\n**Neck部分的整体功能的详细分析如下:**\n\n1\\. Neck的作用：  \nNeck部分在YOLOv8模型中**负责对来自Backbone的特征进行进一步处理和融合**，以提高目标检测的准确性和鲁棒性。它通过引入不同的结构和技术，将多尺度的特征图进行融合，以便更好地捕捉不同尺度目标的信息。\n\n2\\. 特征金字塔网络（Feature Pyramid Network, FPN）：  \nYOLOv8的Neck部分通常采用特征金字塔网络结构，用于处理来自Backbone的多尺度特征图。**FPN通过在不同层级上建立特征金字塔**，使得模型能够在不同尺度上进行目标检测。它通过上采样和下采样操作，将低层级的细节特征与高层级的语义特征进行融合，以获取更全面和丰富的特征表示。\n\n3\\. 特征融合（Feature Fusion）：  \nNeck部分还包括特征融合的操作，**用于将来自不同层级的特征进行融合**。这种特征融合有助于提高模型对目标的检测准确性，尤其是对于不同尺度的目标。\n\n4\\. 上采样和下采样：  \nNeck部分通常会使用上采样和下采样操作，以调整特征图的尺度和分辨率。上采样操作可以将低分辨率的特征图放大到与高分辨率特征图相同的尺寸，**以保留更多的细节信息**。而下采样操作则可以将高分辨率的特征图降低尺寸，**以减少计算量和内存消耗**。\n\nYOLOv8的Neck部分通过特征金字塔网络和特征融合等操作，**有效地提取和融合多尺度的特征**，从而提高了目标检测的性能和鲁棒性。这使得模型能够更好地适应不同尺度和大小的目标，并在复杂场景下取得更准确的检测结果。\n\n> PAN-FPN（具有特征金字塔网络的路径聚合网络）是一种用于计算机视觉中对象检测的神经网络架构。它将特征金字塔网络（FPN）与路径聚合网络（PAN）相结合，以提高目标检测的准确性和效率。\n> \n> FPN 用于从不同比例的图像中提取特征，而 PAN 用于跨网络的不同层聚合这些特征。这允许网络检测不同大小和分辨率的对象，并处理具有多个对象的复杂场景。\n\n#### 3.3 Head\n\n如果Backbone和Neck部分可以理解为准备工作，那么Head部分就是收获的部分，经过前面的准备工作我们得到了Neck部分的输出T1、T2、T3分别代表不同层级的特征图，**Head部分就是对这三个特征图进行处理以产生模型的的输出结果的一个过程。**\n\nYOLOv8的Head部分我们先来看一下它的网络结构。\n\n![bc05b2293026433985d4152e8a116634.png](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/bc05b2293026433985d4152e8a116634.png)\n\n可以看到在YOLOv8的Head部分，体现了最核心的改动——>解耦头(Decoupled-Head)，顾名思义就是将原先的一个检测头分解成两个部分。\n\n在Head部分的三个解耦头分别对应着Neck部分的特征图输出T1、T2、T3。、\n\n**解耦头的工作流程是：**\n\n> 将网络得到的特征图T1，T2，T3分别输入解耦头头进行预测，检测头的结构如下图所示其中包含4个3×3卷积与2个1×1卷积，同时在检测头的回归分支中添加WIOU损失函数如图4所示，回归头部需要计算预测框与真实框之间的位置偏移量，然后将偏移量送入回归头部进行损失计算，然后输出一个四维向量，分别表示目标框的左上角坐标x、y和右下角坐标x、y。分类头部针对于每个Anchor Free提取的候选框对其进行RoI Pooling和卷积操作得到一个分类器输出张量每个位置上的值表示该候选框属于每个类别的概率，在最后通过极大值抑制方式筛选出最终的检测结果 \n\n![ddf8b2f464a348868513ba3488ece02b.png](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/ddf8b2f464a348868513ba3488ece02b.png)\n\n我们再从YOLOv8的yaml文件来看Head部分的作用\n\n![16099ed50b934e5ba7100ab8a381c1cd.png](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/16099ed50b934e5ba7100ab8a381c1cd.png)\n\n**可以看到检测头部分的输出为15,18，21分别对应着Neck部分的三个输出特征图。** \n\n到此YOLOv8的网络结构部分讲解就已经完成，下面我们来看如何利用YOLOv8进行训练操作。  \n\n* * *\n\n四、环境搭建\n------\n\n在我们配置好环境之后，在之后模型获取完成之后，我们可以进行配置的安装我们可以在命令行下输入如下命令进行环境的配置。\n\n```python\npip install -r requirements.txt\n```\n\n输入如上命令之后我们就可以看到命令行在安装模型所需的库了。 \n\n五、数据集获取\n-------\n\n我在上面随便下载了一个 数据集用它导出yolov8的数据集，以及自动给转换成txt的格式yaml文件也已经配置好了，我们直接用就可以。 \n\n![8673527d34eb42348770158c69de678f.png](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/8673527d34eb42348770158c69de678f.png)\n\n* * *\n\n六、模型获取\n------\n\n到这里假设你已经搭建好了环境和有了数据集，那么我们就可以进行模型的下载，因为yolov8目前还存在BUG并不稳定随时都有可能进行更新，所以不推荐大家通过其它的途径下载，最好通过下面的方式进行下载。\n\n我们可以直接在终端命令下\n\n**(PS：这里需要注意的是我们需要在你总项目文件目录下输入这个命令，因为他会下载到当前目录下)**\n\n```python\n pip install ultralytics\n```\n\n 如果大家去github上直接下载zip文件到本地可能会遇到报错如下，识别不了yolo命令，所以推荐大家用这种方式下载，\n\n![c89d06161cd149c0ac0488e90188bcfc.png](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/c89d06161cd149c0ac0488e90188bcfc.png)\n\n* * *\n\n七、模型训练\n------\n\n我们来看一下主要的ultralytics目录结构，\n\n![a900c1d0d16f45e2b8c3829aec6c2499.png](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a900c1d0d16f45e2b8c3829aec6c2499.png)\n\n 我门打开cfg目录下的default.yaml文件可以配置模型的参数，\n\n在其中和模型训练有关的参数及其解释如下:\n\n|  | 参数名 | 输入类型 | 参数解释 |\n| --- | --- | --- | --- |\n| 0 | task | str | YOLO模型的任务选择，选择你是要进行检测、分类等操作 |\n| 1 | mode | str | YOLO模式的选择，选择要进行训练、推理、输出、验证等操作 |\n| 2 | model | str/optional | 模型的文件，可以是官方的预训练模型，也可以是训练自己模型的yaml文件 |\n| 3 | data | str/optional | 模型的地址，可以是文件的地址，也可以是配置好地址的yaml文件 |\n| 4 | epochs | int | 训练的轮次，将你的数据输入到模型里进行训练的次数 |\n| 5 | patience | int | 早停机制，当你的模型精度没有改进了就提前停止训练 |\n| 6 | batch | int | 我们输入的数据集会分解为多个子集，一次向模型里输入多少个子集 |\n| 7 | imgsz | int/list | 输入的图片的大小，可以是整数就代表图片尺寸为int\\*int，或者list分别代表宽和高\\[w，h\\] |\n| 8 | save | bool | 是否保存模型以及预测结果 |\n| 9 | save\\_period | int | 在训练过程中多少次保存一次模型文件,就是生成的pt文件 |\n| 10 | cache | bool | 参数cache用于控制是否启用缓存机制。 |\n| 11 | device | int/str/list/optional | GPU设备的选择：cuda device=0 or device=0,1,2,3 or device=cpu |\n| 12 | workers | int | 工作的线程，Windows系统一定要设置为0否则很可能会引起线程报错 |\n| 13 | name | str/optional | 模型保存的名字，结果会保存到'project/name' 目录下 |\n| 14 | exist\\_ok | bool | 如果模型存在的时候是否进行覆盖操作 |\n| 15 | prepetrained |||\n|||||\n\n### 7.1 训练的三种方式\n\n#### 7.1.1 方式一\n\n我们可以通过命令直接进行训练在其中指定参数，但是这样的方式，我们每个参数都要在其中打出来。命令如下:\n\n```python\nyolo task=detect mode=train model=yolov8n.pt data=data.yaml batch=16 epochs=100 imgsz=640 workers=0 device=0\n```\n\n需要注意的是如果你是Windows系统的电脑其中的Workers最好设置成0否则容易报线程的错误。\n\n#### **7.1.2 方式二（推荐）**\n\n通过指定cfg直接进行训练，我们配置好ultralytics/cfg/default.yaml这个文件之后，可以直接执行这个文件进行训练，这样就不用在命令行输入其它的参数了。\n\n```python\nyolo cfg=ultralytics/cfg/default.yaml\n```\n\n#### **7.1.3 方式三** \n\n 我们可以通过创建py文件来进行训练，这样的好处就是不用在终端上打命令，这也能省去一些工作量，我们在根目录下创建一个名字为run.py的文件，在其中输入代码\n\n```python\nfrom ultralytics import YOLO model = YOLO(\"权重的地址\") data = \"文件的地址\" model.train(data=data, epochs=100, batch=16)\n```\n\n 无论通过上述的哪一种方式在控制台输出如下图片的内容就代表着开始训练成功了！\n\n![4f1fbc25c60f44bd980ee215b5866d12.png](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/4f1fbc25c60f44bd980ee215b5866d12.png)\n\n* * *\n\n八、模型验证/测试 \n----------\n\n|  | 参数名 | 类型 | 参数讲解 |\n| --- | --- | --- | --- |\n| 1 | val | bool | 用于控制是否在训练过程中进行验证/测试。 |\n| 2 | split | str | 用于指定用于验证/测试的数据集划分。可以选择 'val'、'test' 或 'train' 中的一个作为验证/测试数据集 |\n| 3 | save\\_json | bool | 用于控制是否将结果保存为 JSON 文件 |\n| 4 | save\\_hybird | bool | 用于控制是否保存标签和附加预测结果的混合版本 |\n| 5 | conf | float/optional | 用于设置检测时的目标置信度阈值 |\n| 6 | iou | float | 用于设置非极大值抑制（NMS）的交并比（IoU）阈值。 |\n| 7 | max\\_det | int | 用于设置每张图像的最大检测数。 |\n| 8 | half | bool | 用于控制是否使用半精度（FP16）进行推断。 |\n| 9 | dnn | bool | ，用于控制是否使用 OpenCV DNN 进行 ONNX 推断。 |\n| 10 | plots | bool | 用于控制在训练/验证过程中是否保存绘图结果。 |\n\n 验证我们划分的验证集/测试集的情况，也就是评估我们训练出来的best.pt模型好与坏\n\n```python\nyolo task=detect mode=val model=best.pt data=data.yaml device=0\n```\n\n* * *\n\n九、模型推理\n------\n\n我们训练好自己的模型之后，都会生成一个模型文件,保存在你设置的目录下,当我们再次想要实验该模型的效果之后就可以调用该模型进行推理了，我们也可以用官方的预训练权重来进行推理。\n\n推理的方式和训练一样我们这里就选一种来进行举例其它的两种方式都是一样的操作只是需要改一下其中的一些参数即可:\n\n**参数讲解**\n\n|  | 参数名 | 类型 | 参数讲解 |\n| --- | --- | --- | --- |\n| 0 | source | str/optinal | 用于指定图像或视频的目录 |\n| 1 | show | bool | 用于控制是否在可能的情况下显示结果 |\n| 2 | save\\_txt | bool | 用于控制是否将结果保存为 `.txt` 文件 |\n| 3 | save\\_conf | bool | 用于控制是否在保存结果时包含置信度分数 |\n| 4 | save\\_crop | bool | 用于控制是否将带有结果的裁剪图像保存下来 |\n| 5 | show\\_labels | bool | 用于控制在绘图结果中是否显示目标标签 |\n| 6 | show\\_conf | bool | 用于控制在绘图结果中是否显示目标置信度分数 |\n| 7 | vid\\_stride | int/optional | 用于设置视频的帧率步长 |\n| 8 | stream\\_buffer | bool | 用于控制是否缓冲所有流式帧（True）或返回最新的帧（False） |\n| 9 | line\\_width | int/list\\[int\\]/optional | 用于设置边界框的线宽度，如果缺失则自动设置 |\n| 10 | visualize | bool | 用于控制是否可视化模型的特征 |\n| 11 | augment | bool | 用于控制是否对预测源应用图像增强 |\n| 12 | agnostic\\_nms | bool | 用于控制是否使用无关类别的非极大值抑制（NMS） |\n| 13 | classes | int/list\\[int\\]/optional | 用于按类别筛选结果 |\n| 14 | retina\\_masks | bool | 用于控制是否使用高分辨率分割掩码 |\n| 15 | boxes | bool | 用于控制是否在分割预测中显示边界框。 |\n\n```python\nyolo task=detect mode=predict model=best.pt source=images device=0\n```\n\n 这里需要需要注意的是我们用模型进行推理的时候可以选择照片也可以选择一个视频的格式都可以。支持的视频格式有 \n\n> *   MP4（.mp4）：这是一种常见的视频文件格式，通常具有较高的压缩率和良好的视频质量\n>     \n> *   AVI（.avi）：这是一种较旧但仍广泛使用的视频文件格式。它通常具有较大的文件大小\n>     \n> *   MOV（.mov）：这是一种常见的视频文件格式，通常与苹果设备和QuickTime播放器相关\n>     \n> *   MKV（.mkv）：这是一种开放的多媒体容器格式，可以容纳多个视频、音频和字幕轨道\n>     \n> *   FLV（.flv）：这是一种用于在线视频传输的流式视频文件格式\n>     \n\n* * *\n\n十、模型输出\n------\n\n当我们进行部署的时候可以进行文件导出，然后在进行部署。\n\nYOLOv8支持的输出格式有如下\n\n> 1\\. ONNX（Open Neural Network Exchange）：ONNX 是一个开放的深度学习模型表示和转换的标准。它允许在不同的深度学习框架之间共享模型，并支持跨平台部署。导出为 ONNX 格式的模型可以在支持 ONNX 的推理引擎中进行部署和推理。\n> \n> 2\\. TensorFlow SavedModel：TensorFlow SavedModel 是 TensorFlow 框架的标准模型保存格式。它包含了模型的网络结构和参数，可以方便地在 TensorFlow 的推理环境中加载和使用。\n> \n> 3\\. PyTorch JIT（Just-In-Time）：PyTorch JIT 是 PyTorch 的即时编译器，可以将 PyTorch 模型导出为优化的 Torch 脚本或 Torch 脚本模型。这种格式可以在没有 PyTorch 环境的情况下进行推理，并且具有更高的性能。\n> \n> 4\\. Caffe Model：Caffe 是一个流行的深度学习框架，它使用自己的模型表示格式。导出为 Caffe 模型的文件可以在 Caffe 框架中进行部署和推理。\n> \n> 5\\. TFLite（TensorFlow Lite）：TFLite 是 TensorFlow 的移动和嵌入式设备推理框架，支持在资源受限的设备上进行高效推理。模型可以导出为 TFLite 格式，以便在移动设备或嵌入式系统中进行部署。\n> \n> 6\\. Core ML（Core Machine Learning）：Core ML 是苹果的机器学习框架，用于在 iOS 和 macOS 上进行推理。模型可以导出为 Core ML 格式，以便在苹果设备上进行部署。\n> \n> 这些格式都提供了不同的优势和适用场景。选择合适的导出格式应该考虑到目标平台和部署环境的要求，以及所使用的深度学习框架的支持情况。\n\n模型输出的参数有如下\n\n|  | 参数名 | 类型 | 参数解释 |\n| --- | --- | --- | --- |\n| 0 | format | str | 导出模型的格式 |\n| 1 | keras | bool | 表示是否使用Keras |\n| 2 | optimize | bool | 用于在导出TorchScript模型时进行优化，以便在移动设备上获得更好的性能 |\n| 3 | int8 | bool | 用于在导出CoreML或TensorFlow模型时进行INT8量化 |\n| 4 | dynamic | bool | 用于在导出CoreML或TensorFlow模型时进行INT8量化 |\n| 5 | simplify | bool | 用于在导出ONNX模型时进行模型简化 |\n| 6 | opset | int/optional | 用于指定导出ONNX模型时的opset版本 |\n| 7 | workspace | int | 用于指定TensorRT模型的工作空间大小，以GB为单位 |\n| 8 | nms | bool | 用于在导出CoreML模型时添加非极大值抑制（NMS） |\n\n命令行命令如下: \n\n```python\nyolo task=detect mode=export model=best.pt format=onnx  \n```\n"},{"title":"可视化热力图","tags":["python"],"categories":["YOLO"],"author":"Louaq","excerpt":"\n\n\n一、本文介绍\n------\n\n本文给大家带来的机制是的**可视化热力图功能**，热力图)作为我们论文当中的必备一环，可以展示出我们呈现机制的有效性，本文的内容支持YOLOv8最新版本，同时支持视","link":"/posts/article_10","content":"\n\n\n一、本文介绍\n------\n\n本文给大家带来的机制是的**可视化热力图功能**，热力图)作为我们论文当中的必备一环，可以展示出我们呈现机制的有效性，本文的内容支持YOLOv8最新版本，同时支持视频讲解，本文的内容是根据检测头的输出内容，然后来绘图，**产生6300张预测图片，从中选取出有效的热力图来绘图。**\n\n在开始之前给大家推荐一下我的专栏，本专栏每周更新3-10篇最新前沿机制 | 包括二次创新全网无重复，以及融合改进(大家拿到之后添加另外一个改进机制在你的数据集上实现涨点即可撰写论文)，还有各种前沿顶会改进机制 |，更有包含我所有附赠的文件（文件内集成我所有的改进机制全部注册完毕可以直接运行）和交流群和视频讲解提供给大家。  \n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0a203fd9e3cd4f89b95e419e4fc22f1f.png)\n\n\n\n* * *\n\n二、项目完整代码 \n---------\n\n**我们将这个代码，复制粘贴到我们YOLOv8的仓库里然后创建一个py文件存放进去即可。**\n\n```python\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')\nimport torch, yaml, cv2, os, shutil\nimport numpy as np\nnp.random.seed(0)\nimport matplotlib.pyplot as plt\nfrom tqdm import trange\nfrom PIL import Image\nfrom ultralytics.nn.tasks import DetectionModel as Model\nfrom ultralytics.utils.torch_utils import intersect_dicts\nfrom ultralytics.utils.ops import xywh2xyxy\nfrom pytorch_grad_cam import GradCAMPlusPlus, GradCAM, XGradCAM\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\nfrom pytorch_grad_cam.activations_and_gradients import ActivationsAndGradients\n \ndef letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n    # Resize and pad image while meeting stride-multiple constraints\n    shape = im.shape[:2]  # current shape [height, width]\n    if isinstance(new_shape, int):\n        new_shape = (new_shape, new_shape)\n \n    # Scale ratio (new / old)\n    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n        r = min(r, 1.0)\n \n    # Compute padding\n    ratio = r, r  # width, height ratios\n    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n    if auto:  # minimum rectangle\n        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n    elif scaleFill:  # stretch\n        dw, dh = 0.0, 0.0\n        new_unpad = (new_shape[1], new_shape[0])\n        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n \n    dw /= 2  # divide padding into 2 sides\n    dh /= 2\n \n    if shape[::-1] != new_unpad:  # resize\n        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n    return im, ratio, (dw, dh)\n \nclass yolov8_heatmap:\n    def __init__(self, weight, cfg, device, method, layer, backward_type, conf_threshold, ratio):\n        device = torch.device(device)\n        ckpt = torch.load(weight)\n        model_names = ckpt['model'].names\n        csd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32\n        model = Model(cfg, ch=3, nc=len(model_names)).to(device)\n        csd = intersect_dicts(csd, model.state_dict(), exclude=['anchor'])  # intersect\n        model.load_state_dict(csd, strict=False)  # load\n        model.eval()\n        print(f'Transferred {len(csd)}/{len(model.state_dict())} items')\n        \n        target_layers = [eval(layer)]\n        method = eval(method)\n \n        colors = np.random.uniform(0, 255, size=(len(model_names), 3)).astype(np.int32)\n        self.__dict__.update(locals())\n    \n    def post_process(self, result):\n        logits_ = result[:, 4:]\n        boxes_ = result[:, :4]\n        sorted, indices = torch.sort(logits_.max(1)[0], descending=True)\n        return torch.transpose(logits_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]], xywh2xyxy(torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]]).cpu().detach().numpy()\n    \n    def draw_detections(self, box, color, name, img):\n        xmin, ymin, xmax, ymax = list(map(int, list(box)))\n        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), tuple(int(x) for x in color), 2)\n        cv2.putText(img, str(name), (xmin, ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, tuple(int(x) for x in color), 2, lineType=cv2.LINE_AA)\n        return img\n \n    def __call__(self, img_path, save_path):\n        # remove dir if exist\n        if os.path.exists(save_path):\n            shutil.rmtree(save_path)\n        # make dir if not exist\n        os.makedirs(save_path, exist_ok=True)\n \n        # img process\n        img = cv2.imread(img_path)\n        img = letterbox(img)[0]\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = np.float32(img) / 255.0\n        tensor = torch.from_numpy(np.transpose(img, axes=[2, 0, 1])).unsqueeze(0).to(self.device)\n \n        # init ActivationsAndGradients\n        grads = ActivationsAndGradients(self.model, self.target_layers, reshape_transform=None)\n \n        # get ActivationsAndResult\n        result = grads(tensor)\n        activations = grads.activations[0].cpu().detach().numpy()\n \n        # postprocess to yolo output\n        post_result, pre_post_boxes, post_boxes = self.post_process(result[0])\n        for i in trange(int(post_result.size(0) * self.ratio)):\n            if float(post_result[i].max()) < self.conf_threshold:\n                break\n \n            self.model.zero_grad()\n            # get max probability for this prediction\n            if self.backward_type == 'class' or self.backward_type == 'all':\n                score = post_result[i].max()\n                score.backward(retain_graph=True)\n \n            if self.backward_type == 'box' or self.backward_type == 'all':\n                for j in range(4):\n                    score = pre_post_boxes[i, j]\n                    score.backward(retain_graph=True)\n \n            # process heatmap\n            if self.backward_type == 'class':\n                gradients = grads.gradients[0]\n            elif self.backward_type == 'box':\n                gradients = grads.gradients[0] + grads.gradients[1] + grads.gradients[2] + grads.gradients[3]\n            else:\n                gradients = grads.gradients[0] + grads.gradients[1] + grads.gradients[2] + grads.gradients[3] + grads.gradients[4]\n            b, k, u, v = gradients.size()\n            weights = self.method.get_cam_weights(self.method, None, None, None, activations, gradients.detach().numpy())\n            weights = weights.reshape((b, k, 1, 1))\n            saliency_map = np.sum(weights * activations, axis=1)\n            saliency_map = np.squeeze(np.maximum(saliency_map, 0))\n            saliency_map = cv2.resize(saliency_map, (tensor.size(3), tensor.size(2)))\n            saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n            if (saliency_map_max - saliency_map_min) == 0:\n                continue\n            saliency_map = (saliency_map - saliency_map_min) / (saliency_map_max - saliency_map_min)\n \n            # add heatmap and box to image\n            cam_image = show_cam_on_image(img.copy(), saliency_map, use_rgb=True)\n            \"不想在图片中绘画出边界框和置信度，注释下面的一行代码即可\"\n            cam_image = self.draw_detections(post_boxes[i], self.colors[int(post_result[i, :].argmax())], f'{self.model_names[int(post_result[i, :].argmax())]} {float(post_result[i].max()):.2f}', cam_image)\n            cam_image = Image.fromarray(cam_image)\n            cam_image.save(f'{save_path}/{i}.png')\n \ndef get_params():\n    params = {\n        'weight': 'yolov8n.pt',   # 训练出来的权重文件\n        'cfg': 'ultralytics/cfg/models/v8/yolov8n.yaml',  # 训练权重对应的yaml配置文件\n        'device': 'cuda:0',\n        'method': 'GradCAM', # GradCAMPlusPlus, GradCAM, XGradCAM , 使用的热力图库文件不同的效果不一样可以多尝试\n        'layer': 'model.model[9]',  # 想要检测的对应层\n        'backward_type': 'all', # class, box, all\n        'conf_threshold': 0.01, # 0.6  # 置信度阈值，有的时候你的进度条到一半就停止了就是因为没有高于此值的了\n        'ratio': 0.02 # 0.02-0.1\n    }\n    return params\n \nif __name__ == '__main__':\n    model = yolov8_heatmap(**get_params())\n    model(r'ultralytics/assets/bus.jpg', 'result')  # 第一个是检测的文件, 第二个是保存的路径\n```\n\n 三、参数解析 \n--------\n\n**下面上面项目核心代码的参数解析，共有7个，能够起到作用的参数并不多。** \n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/6fcc9bb6ec814ab1961efb0c7db5f1a1.png)\n\n|  | 参数名 | 参数类型 | 参数讲解 |\n| --- | --- | --- | --- |\n| 0 | weights |  str | 用于检测视频的权重文件地址（可以是你训练好的，也可以是官方提供的） |\n| 1 | cfg | str | 你选择的权重对应的yaml配置文件，请注意一定要对应否则会报错和不显示图片 |\n| 2 | device | str | 设备的选择可以用GPU也可以用CPU |\n| 3 | method | str | 使用的热力图第三方库的版本，不同的版本效果也不一样。 |\n| 4 | layer | str | 想要检测的对应层，比如这里设置的是9那么检测的就是第九层 |\n| 4 | backward\\_type | str | 检测的类别 |\n| 5 | conf\\_threshold | str | 置信度阈值，有的时候你的进度条没有满就是因为没有大于这个阈值的图片了 |\n| 6 | ratio | int | YOLOv8一次产生6300张预测框，选择多少比例的图片绘画热力图。 |\n\n四、项目的使用教程\n---------\n\n### 4.1 步骤一\n\n我们在Yolo仓库的目录下创建一个py文件将代码存放进去，如下图所示。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/6c4bed4cef9a488d8cda62492efa80c7.png)\n\n* * *\n\n### 4.2 步骤二\n\n**我们按照参数解析部分的介绍填好大家的参数，主要配置的有两个一个就是权重文件地址另一个就是图片的地址。**\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/2bd70b825206498bb8688b90cfa3e12c.png)\n\n* * *\n\n### 4.3 步骤三\n\n我们挺好之后运行文件即可，图片就会保存在同级目录下的新的文件夹result内。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0a203fd9e3cd4f89b95e419e4fc22f1f.png)\n\n* * *\n\n### 4.4 置信度和检测框 \n\n看下下面的说明就行。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/26206bf8e8cb4da4b2f230ac932cd577.png)\n\n* * *\n\n"},{"title":"评估","tags":["python"],"categories":["深度学习"],"author":"Louaq","excerpt":"\n一、简介\n----\n\n这篇博客，**主要给大家讲解我们在训练yolov8时生成的结果文件中各个图片及其中指标的含义**，帮助大家更深入的理解，以及我们在评估模型时和发表论文时主要关注的参数有那些。本","link":"/posts/article_11","content":"\n一、简介\n----\n\n这篇博客，**主要给大家讲解我们在训练yolov8时生成的结果文件中各个图片及其中指标的含义**，帮助大家更深入的理解，以及我们在评估模型时和发表论文时主要关注的参数有那些。本文通过举例训练过程中的某一时间的结果来帮助大家理解，大家阅读过程中如有任何问题可以在评论区提问出来，我会帮助大家解答。首先我们来看一个在一次训练完成之后都能生成多少个文件如下图所示，下面的文章讲解都会围绕这个结果文件来介绍。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/bdf99b744c6646f6a82b2be30e3e9d92.png)\n\n* * *\n\n**二、评估用的数据集** \n--------------\n\n> 上面的训练结果，是根据一个检测飞机的数据集训练得来，其中只有个标签就是飞机，对于这种单标签的数据集，其实我们可以将其理解为一个二分类任务，\n> \n> **一种情况->检测为飞机，另一种情况->不是飞机。**\n\n* * *\n\n三、结果分析 \n-------\n\n我们可以从结果文件中看到其中**共有文件24个**，后12张图片是根据我们训练过程中的一些检测结果图片，用于我们可以观察检测结果，有哪些被检测出来了，那些没有被检测出来，其不作为指标评估的文件。         \n\n### Weights文件夹\n\n我们先从第一个weights文件夹来分析，其中有两个文件，分别是**best.pt、last.pt**,其分别为训练过程中的损失最低的结果和模型训练的最后一次结果保存的模型。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/3986c306bb3b4e9893da7f89d2994a88.png)\n\n### args.yaml\n\n第二个文件是args.yaml文件,其中主要保存一些我们训练时指定的参数，内容如下所示。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/f464e438dd6f4f0a9c52e7246439295c.png)\n\n### 混淆矩阵(ConfusionMatrix)\n\n第三个文件就是混淆矩阵，大家都应该听过这个名字，其是一种用于评估分类模型性能的表格形式。它以实际类别（真实值）和模型预测类别为基础，将样本分类结果进行统计和汇总。\n\n> 对于二分类问题，混淆矩阵通常是一个2×2的矩阵，包括真阳性（True Positive, TP）、真阴性（True Negative, TN）、假阳性（False Positive, FP）和假阴性（False Negative, FN）四个元素。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/ae117a5a660142f3a44b52834fa04ec3.png)\n\n```cobol\nTrue_Label = [1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1 ,0, 1, 0 , 1 , 0, 0 , 1]Predict_Label = [0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1 ,0 , 0 , 1 , 0, 0 , 1, 0]\n```\n\n我们来分析这个图，其每个格子代表的含义我在图片上标注了出来**,下面我们来拿一个例子来帮助大家来理解这个混淆矩阵。**\n\n假设我们的数据集预测为飞机标记为数字0、预测不为飞机标记为1，**现在假设我们在模型的训练的某一批次种预测了20次其真实结果和预测结果如下所示。** \n\n其中True\\_Label代表真实的标签，Predict\\_Label代表我们用模型预测的标签。\n\n那么我们可以进行对比产生如下分析\n\n> *   6个样本的真实标签和预测标签都是0（真阴性，True Negative）。\n> *   1个样本的真实标签是0，但预测标签是1（假阳性，False Positive）。\n> *   8个样本的真实标签是1，但预测标签是0（假阴性，False Negative）。\n> *   5个样本的真实标签和预测标签都是1（真阳性，True Positive）。\n\n下面根据我们的分析结果，我们就能够画出这个预测的混淆矩阵，\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/36c503208f654d06a1ad585e772364a8.png)\n\n由此我们就能得到那一批次的混淆矩阵，**我们的最终结果生成的混淆矩阵可以理解为多个混淆矩阵的统计结果。** \n\n### 混淆矩阵归一化(Confusion Matrix Normal)\n\n这个混淆矩阵的归一化，就是对混淆矩阵做了一个归一化处理，对混淆矩阵进行归一化可以将每个单元格的值除以该类别实际样本数，从而得到表示分类准确率的百分比。这种标准化使得我们可以直观地比较类别间的分类准确率，并识别出模型在哪些类别上表现较好或较差。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/4642ed3defe146a3b93999ffbd5d5129.png)\n\n**我们可以看到是对于列进行了归一化处理，0.9 + 0.1 = 1，1 + 0 = 1。** \n\n### **计算mAP、Precision、Recall**\n\n在讲解其它的图片之前我们需要来计算三个比较重要的参数，这是其它图片的基础，这里的计算还是利用上面的某一批次举例的分析结果。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/09e2217c78ab4e6ab49eeb2b8f128fed.png)\n\n1.  精确度（Precision）：**预测为正的样本中有多少是正确的**，Precision = TP / (TP + FP) = 5 / (5 + 1) = 5/6 ≈ 0.833\n    \n2.  召回率（Recall）：真实为正的样本中有多少被正确预测为正，Recall = TP / (TP + FN) = 5 / (5 + 8) ≈ 0.385\n    \n3.  F1值（F1-Score）：**综合考虑精确度和召回率的指标，**F1 = 2 \\* (Precision \\* Recall) / (Precision + Recall) = 2 \\* (0.833 \\* 0.385) / (0.833 + 0.385) ≈ 0.526\n    \n4.  准确度（Accuracy）：**所有样本中模型正确预测的比例，**Accuracy = (TP + TN) / (TP + TN + FP + FN) = (5 + 6) / (5 + 6 + 1 + 8) ≈ 0.565\n    \n5.  平均精确度（Average Precision, AP）：**用于计算不同类别的平均精确度，对于二分类问题，AP等于精确度。**AP = Precision = 0.833\n    \n6.  平均精确度（Mean Average Precision, mAP）：**多类别问题的平均精确度，对于二分类问题，mAP等于AP（精确度）**，所以mAP = AP = 0.833\n    \n\n这里需要讲解的主要是AP和MAP如果是多分类的问题，AP和mAP怎么计算，首先我们要知道AP的全称就是Average Precision，平均精度所以我们AP的计算公式如下？\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/5a9f270d50ce4bbfb76e800f4553200c.png)\n\nmAP就是Mean Average Precision，计算如下，计算每一个没别的AP进行求平均值处理就是mAP。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a7ef1cd0cb924112acfa07084524a7a8.png)\n\n### F1\\_Curve \n\nF1\\_Curve这个文件，我们点击去的图片的标题是F1-Confidence Curve它显示了在不同分类阈值下的F1值变化情况。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/97f8c2e20dd24d59a954bd43c4644c0f.png)\n\n我们可以这么理解，先看它的横纵坐标，横坐标是置信度，纵坐标是F1-Score，F1-Score在前面我们以及讲解过了，那什么是置信度？\n\n**置信度(Confidence)->**在我们模型的识别过程中会有一个概率，就是模型判定一个物体并不是百分百判定它是属于某一个分类，它会给予它以个概率，Confidence就是我们设置一个阈值，如果超过这个概率那么就确定为某一分类，**假如我模型判定一个物体由0.7的概率属于飞机，此时我们设置的阈值如果为0.7以下那么模型就会输出该物体为飞机，如果我们设置的阈值大于0.7那么模型就不会输出该物体为飞机。**\n\n**F1-Confidence Curve就是随着F1-Score随着Confience的逐渐增高而变化的一个曲线。**\n\n### Labels\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/521ff0b11be64fcbbbd711c3de43ddcd.jpeg)\n\nLabels图片代表每个检测到的目标的类别和边界框信息。每个目标都由一个矩形边界框和一个类别标签表示，**我们逆时针来看这个图片！！！**\n\n1.  目标类别：该像素点所检测到的目标类别，例如飞机等。\n2.  目标位置：该像素点所检测到的目标在图像中的位置，即该像素点在图像中的坐标。\n3.  目标大小：该像素点所检测到的目标的大小，即该像素点所覆盖的区域的大小。\n4.  其他信息：例如目标的旋转角度等其他相关信息。\n\n### labels\\_correlogram\n\nlabels\\_correlogram是一个在**机器学习领域中使用的术语，**它指的是一种图形，**用于显示目标检测算法在训练过程中预测标签之间的相关性**。\n\n具体来说，labels\\_correlogram是一张**颜色矩阵图**，它展示了训练集数据标签之间的相关性。它可以帮助我们理解目标检测算法在训练过程中的行为和表现，以及预测标签之间的相互影响。\n\n通过观察labels\\_correlogram，我们可以了解到目标检测算法在不同类别之间的区分能力，以及对于不同类别的预测精度。此外，我们还可以通过比较不同算法或不同数据集labels\\_correlogram，来评估算法的性能和数据集的质量。\n\n总之，labels\\_correlogram是一种有用的工具，可以帮助我们更好地理解目标检测算法在训练过程中的行为和表现，以及评估算法的性能和数据集的质量。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0f1e5f82a532423dae3a4e8b897e6165.jpeg)\n\n### P\\_curve \n\n这个图的分析和F1\\_Curve一样，不同的是关于的是Precision和Confidence之间的关系，**可以看出我们随着置信度的越来越高检测的准确率按理来说是越来越高的。** \n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/7ac794c6f34b418c95dfc7951382171c.png)\n\n### R\\_curve \n\n这个图的分析和F1\\_Curve一样，不同的是关于的是Recall和Confidence之间的关系，**可以看出我们随着置信度的越来越高召回率的准确率按理来说是越来越低的。** \n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/e72c4546e65d445c9831567e12d55df0.png)\n\n### PR\\_curve\n\n它显示了在不同分类阈值下模型的精确度（Precision）和召回率（Recall）之间的关系。\n\n**PR曲线越靠近坐标轴的右上角，模型性能越好，越能够正确识别正样本，正确分类正样本的Precision值越高，而靠近右侧则说明模型对正样本的识别能力较差，即召回能力较差。**\n\n> PR曲线的特点是随着分类阈值的变化，精确度和召回率会有相应的改变。通常情况下，当分类模型能够同时保持较高的精确度和较高的召回率时，PR曲线处于较高的位置。当模型偏向于高精确度或高召回率时，曲线则相应地向低精确度或低召回率的方向移动。\n> \n> PR曲线可以帮助我们评估模型在不同阈值下的性能，并选择适当的阈值来平衡精确度和召回率。对于模型比较或选择，我们可以通过比较PR曲线下方的面积（称为平均精确度均值，Average Precision, AP）来进行定量评估。AP值越大，模型的性能越好。\n> \n> 总结：PR曲线是一种展示分类模型精确度和召回率之间关系的可视化工具，通过绘制精确度-召回率曲线，我们可以评估和比较模型在不同分类阈值下的性能，并计算平均精确度均值（AP）来定量衡量模型的好坏。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/c00378b5866f44978bf907f4b92d6a2c.png)\n\n### results.csv\n\nresults.csv记录了一些我们训练过程中的参数信息，包括损失和学习率等，这里没有什么需要理解大家可以看一看，我们后面的results图片就是根据这个文件绘画出来的。\n\n### ![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/9af828676f704aada0b9b18797ba75ce.png)results\n\n这个图片就是生成结果的最后一个了，我们可以看出其中标注了许多小的图片包括训练过程在的各种损失，我们主要看的其实就是后面的四幅图mAP50、mAP50-95、metrics/precision、metrics/recall四张图片。 \n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0108b195b2e04b46811b44dc9f5f351f.png)\n\n> 1.  mAP50：mAP是mean Average Precision的缩写，表示在多个类别上的平均精度。mAP50表示在50%的IoU阈值下的mAP值。\n> 2.  mAP50-95：这是一个更严格的评价指标，它计算了在50-95%的IoU阈值范围内的mAP值，然后取平均。这能够更准确地评估模型在不同IoU阈值下的性能。\n> 3.  metrics/precision：精度（Precision）是评估模型预测正确的正样本的比例。在目标检测中，如果模型预测的边界框与真实的边界框重合，则认为预测正确。\n> 4.  metrics/recall：召回率（Recall）是评估模型能够找出所有真实正样本的比例。在目标检测中，如果真实的边界框与预测的边界框重合，则认为该样本被正确召回。\n\n### 检测效果图\n\n 最后的十四张图片就是检测效果图了，给大家看一下这里没什么好讲解的了。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/7dbceef24f184435b49dd7480b2cc2b3.jpeg)\n\n* * *\n\n四、其它参数\n------\n\nFPS和IoU是目标检测领域中使用的两个重要指标，分别表示每秒处理的图片数量和交并比。\n\n> 1.  FPS：全称为Frames Per Second，即每秒帧率。它用于评估模型在给定硬件上的处理速度，即每秒可以处理的图片数量。该指标对于实现实时检测非常重要，因为只有处理速度快，才能满足实时检测的需求。\n> 2.  IoU：全称为Intersection over Union，表示交并比。在目标检测中，它用于衡量模型生成的候选框与原标记框之间的重叠程度。IoU值越大，表示两个框之间的相似性越高。通常，当IoU值大于0.5时，认为可以检测到目标物体。这个指标常用于评估模型在特定数据集上的检测准确度。\n\n在目标检测领域中，处理速度和准确度是两个重要的性能指标。在实际应用中，我们需要根据具体需求来平衡这两个指标。\n\n"},{"title":"绘图","tags":["人工智能","论文","python"],"categories":["写作"],"author":"Louaq","excerpt":"\n\n\n一、本文介绍\n------\n\n本文给大家带来的是**YOLOv8系列的绘图功能**，我将向大家介绍YOLO系列的绘图功能。我们在进行实验时，经常需要比较多个结果，针对这一问题，我写了点代码来解决","link":"/posts/article_12","content":"\n\n\n一、本文介绍\n------\n\n本文给大家带来的是**YOLOv8系列的绘图功能**，我将向大家介绍YOLO系列的绘图功能。我们在进行实验时，经常需要比较多个结果，针对这一问题，我写了点代码来解决这个问题，它可以根据训练结果绘制损失(loss)和mAP（平均精度均值）的对比图。这个工具不仅支持多个文件的对比分析，还允许大家在现有代码的基础上进行修，从而达到数据可视化的功能，大家也可以将对比图来放在论文中进行对比也是非常不错的选择。\n\n**先展示一下效果图->** \n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/059f9bb891c3424aaf012d14e370287b.png)\n\n  \n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/059f9bb891c3424aaf012d14e370287b.png)​\n\n**损失对比图象->**\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0b93636a9f29432fadb3f0aeacd3406c.png)​\n\n\n\n\n\n\n\n* * *\n\n二、绘图工具核心代码 \n-----------\n\n```python\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n \n \ndef plot_metrics_and_loss(experiment_names, metrics_info, loss_info, metrics_subplot_layout, loss_subplot_layout,\n                          metrics_figure_size=(15, 10), loss_figure_size=(15, 10), base_directory='runs/train'):\n    # Plot metrics\n    plt.figure(figsize=metrics_figure_size)\n    for i, (metric_name, title) in enumerate(metrics_info):\n        plt.subplot(*metrics_subplot_layout, i + 1)\n        for name in experiment_names:\n            file_path = os.path.join(base_directory, name, 'results.csv')\n            data = pd.read_csv(file_path)\n            column_name = [col for col in data.columns if col.strip() == metric_name][0]\n            plt.plot(data[column_name], label=name)\n        plt.xlabel('Epoch')\n        plt.title(title)\n        plt.legend()\n    plt.tight_layout()\n    metrics_filename = 'metrics_curves.png'\n    plt.savefig(metrics_filename)\n    plt.show()\n \n    # Plot loss\n    plt.figure(figsize=loss_figure_size)\n    for i, (loss_name, title) in enumerate(loss_info):\n        plt.subplot(*loss_subplot_layout, i + 1)\n        for name in experiment_names:\n            file_path = os.path.join(base_directory, name, 'results.csv')\n            data = pd.read_csv(file_path)\n            column_name = [col for col in data.columns if col.strip() == loss_name][0]\n            plt.plot(data[column_name], label=name)\n        plt.xlabel('Epoch')\n        plt.title(title)\n        plt.legend()\n    plt.tight_layout()\n    loss_filename = 'loss_curves.png'\n    plt.savefig(loss_filename)\n    plt.show()\n \n    return metrics_filename, loss_filename\n \n \n# Metrics to plot\nmetrics_info = [\n    ('metrics/precision(B)', 'Precision'),\n    ('metrics/recall(B)', 'Recall'),\n    ('metrics/mAP50(B)', 'mAP at IoU=0.5'),\n    ('metrics/mAP50-95(B)', 'mAP for IoU Range 0.5-0.95')\n]\n \n# Loss to plot\nloss_info = [\n    ('train/box_loss', 'Training Box Loss'),\n    ('train/cls_loss', 'Training Classification Loss'),\n    ('train/dfl_loss', 'Training DFL Loss'),\n    ('val/box_loss', 'Validation Box Loss'),\n    ('val/cls_loss', 'Validation Classification Loss'),\n    ('val/dfl_loss', 'Validation DFL Loss')\n]\n \n# Plot the metrics and loss from multiple experiments\nmetrics_filename, loss_filename = plot_metrics_and_loss(\n    experiment_names=['exp294', 'exp297', 'exp293', 'exp291', 'exp287'],\n    metrics_info=metrics_info,\n    loss_info=loss_info,\n    metrics_subplot_layout=(2, 2),\n    loss_subplot_layout=(2, 3)\n)\n```\n\n* * *\n\n三、使用讲解 \n-------\n\n使用方式非常简单，我们首先创建一个文件，将核心代码粘贴进去，其中experiment\\_names这个参数就代表我们的每个训练结果的名字， 我们只需要修改这个即可，我这里就是五个结果进行对比，修改完成之后大家运行该文件即可。![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/e7d9ce0ce8004e178f1386272eb6c319.png)​\n\n五、热力图代码 \n--------\n\n使用方式我会单独更一篇，这个热力图代码的进阶版，这里只是先放一下。 \n\n```python\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')\nimport torch, yaml, cv2, os, shutil\nimport numpy as np\nnp.random.seed(0)\nimport matplotlib.pyplot as plt\nfrom tqdm import trange\nfrom PIL import Image\nfrom ultralytics.nn.tasks import DetectionModel as Model\nfrom ultralytics.utils.torch_utils import intersect_dicts\nfrom ultralytics.utils.ops import xywh2xyxy\nfrom pytorch_grad_cam import GradCAMPlusPlus, GradCAM, XGradCAM\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\nfrom pytorch_grad_cam.activations_and_gradients import ActivationsAndGradients\n \ndef letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n    # Resize and pad image while meeting stride-multiple constraints\n    shape = im.shape[:2]  # current shape [height, width]\n    if isinstance(new_shape, int):\n        new_shape = (new_shape, new_shape)\n \n    # Scale ratio (new / old)\n    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n        r = min(r, 1.0)\n \n    # Compute padding\n    ratio = r, r  # width, height ratios\n    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n    if auto:  # minimum rectangle\n        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n    elif scaleFill:  # stretch\n        dw, dh = 0.0, 0.0\n        new_unpad = (new_shape[1], new_shape[0])\n        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n \n    dw /= 2  # divide padding into 2 sides\n    dh /= 2\n \n    if shape[::-1] != new_unpad:  # resize\n        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n    return im, ratio, (dw, dh)\n \nclass yolov8_heatmap:\n    def __init__(self, weight, cfg, device, method, layer, backward_type, conf_threshold, ratio):\n        device = torch.device(device)\n        ckpt = torch.load(weight)\n        model_names = ckpt['model'].names\n        csd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32\n        model = Model(cfg, ch=3, nc=len(model_names)).to(device)\n        csd = intersect_dicts(csd, model.state_dict(), exclude=['anchor'])  # intersect\n        model.load_state_dict(csd, strict=False)  # load\n        model.eval()\n        print(f'Transferred {len(csd)}/{len(model.state_dict())} items')\n        \n        target_layers = [eval(layer)]\n        method = eval(method)\n \n        colors = np.random.uniform(0, 255, size=(len(model_names), 3)).astype(np.int)\n        self.__dict__.update(locals())\n    \n    def post_process(self, result):\n        logits_ = result[:, 4:]\n        boxes_ = result[:, :4]\n        sorted, indices = torch.sort(logits_.max(1)[0], descending=True)\n        return torch.transpose(logits_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]], xywh2xyxy(torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]]).cpu().detach().numpy()\n    \n    def draw_detections(self, box, color, name, img):\n        xmin, ymin, xmax, ymax = list(map(int, list(box)))\n        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), tuple(int(x) for x in color), 2)\n        cv2.putText(img, str(name), (xmin, ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, tuple(int(x) for x in color), 2, lineType=cv2.LINE_AA)\n        return img\n \n    def __call__(self, img_path, save_path):\n        # remove dir if exist\n        if os.path.exists(save_path):\n            shutil.rmtree(save_path)\n        # make dir if not exist\n        os.makedirs(save_path, exist_ok=True)\n \n        # img process\n        img = cv2.imread(img_path)\n        img = letterbox(img)[0]\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = np.float32(img) / 255.0\n        tensor = torch.from_numpy(np.transpose(img, axes=[2, 0, 1])).unsqueeze(0).to(self.device)\n \n        # init ActivationsAndGradients\n        grads = ActivationsAndGradients(self.model, self.target_layers, reshape_transform=None)\n \n        # get ActivationsAndResult\n        result = grads(tensor)\n        activations = grads.activations[0].cpu().detach().numpy()\n \n        # postprocess to yolo output\n        post_result, pre_post_boxes, post_boxes = self.post_process(result[0])\n        for i in trange(int(post_result.size(0) * self.ratio)):\n            if float(post_result[i].max()) < self.conf_threshold:\n                break\n \n            self.model.zero_grad()\n            # get max probability for this prediction\n            if self.backward_type == 'class' or self.backward_type == 'all':\n                score = post_result[i].max()\n                score.backward(retain_graph=True)\n \n            if self.backward_type == 'box' or self.backward_type == 'all':\n                for j in range(4):\n                    score = pre_post_boxes[i, j]\n                    score.backward(retain_graph=True)\n \n            # process heatmap\n            if self.backward_type == 'class':\n                gradients = grads.gradients[0]\n            elif self.backward_type == 'box':\n                gradients = grads.gradients[0] + grads.gradients[1] + grads.gradients[2] + grads.gradients[3]\n            else:\n                gradients = grads.gradients[0] + grads.gradients[1] + grads.gradients[2] + grads.gradients[3] + grads.gradients[4]\n            b, k, u, v = gradients.size()\n            weights = self.method.get_cam_weights(self.method, None, None, None, activations, gradients.detach().numpy())\n            weights = weights.reshape((b, k, 1, 1))\n            saliency_map = np.sum(weights * activations, axis=1)\n            saliency_map = np.squeeze(np.maximum(saliency_map, 0))\n            saliency_map = cv2.resize(saliency_map, (tensor.size(3), tensor.size(2)))\n            saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n            if (saliency_map_max - saliency_map_min) == 0:\n                continue\n            saliency_map = (saliency_map - saliency_map_min) / (saliency_map_max - saliency_map_min)\n \n            # add heatmap and box to image\n            cam_image = show_cam_on_image(img.copy(), saliency_map, use_rgb=True)\n            cam_image = self.draw_detections(post_boxes[i], self.colors[int(post_result[i, :].argmax())], f'{self.model_names[int(post_result[i, :].argmax())]} {float(post_result[i].max()):.2f}', cam_image)\n            cam_image = Image.fromarray(cam_image)\n            cam_image.save(f'{save_path}/{i}.png')\n \ndef get_params():\n    params = {\n        'weight': 'yolov8n.pt',\n        'cfg': 'ultralytics/cfg/models/v8/yolov8n.yaml',\n        'device': 'cuda:0',\n        'method': 'GradCAM', # GradCAMPlusPlus, GradCAM, XGradCAM\n        'layer': 'model.model[9]',\n        'backward_type': 'all', # class, box, all\n        'conf_threshold': 0.6, # 0.6\n        'ratio': 0.02 # 0.02-0.1\n    }\n    return params\n \nif __name__ == '__main__':\n    model = yolov8_heatmap(**get_params())\n    model(r'ultralytics/assets/bus.jpg', 'result')\n```\n\n"},{"title":"数据集","tags":["人工智能"],"categories":["YOLO"],"author":"Louaq","excerpt":"\n\n\nYoloV8。YoloV8是一种高效的目标检测算法，它的训练需要高质量的数据集。然而，获取高质量的数据集是一项耗时且费力的任务。\n\n![](https://yangyang666.oss-cn-","link":"/posts/article_13","content":"\n\n\nYoloV8。YoloV8是一种高效的目标检测算法，它的训练需要高质量的数据集。然而，获取高质量的数据集是一项耗时且费力的任务。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/ad4f0d35d5a24785bdf9b5d0517be144.png)\n\nYoloV8官方推荐了一个数据集网站，就是Roboflow。Roboflow是一个数据集管理平台，提供了免费的数据集，同时也支持上传自己的数据集进行格式转换。使用Roboflow，开发者可以方便地获取所需格式的数据集，无需手动转换格式。此外，Roboflow还提供了多种数据预处理、数据增强等功能，可帮助开发者更好地优化训练数据，**从下面官方获取的图片上来YOLOv8官方指定的数据集获取网站就是Roboflow。**\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/f1d1f09f485f423ba47df662c9c4f451.png)\n\n因此，如果你正在使用YoloV8进行目标检测算法的训练，Roboflow是一个非常好的选择，可以帮助你快速获取高质量的数据集，从而加快训练效率。同时，Roboflow也是YoloV8官方推荐的数据集网站，保证了数据集的质量和可靠性。\n\n### **下面首先分享Roboflow的官方地址**\n\n**[请点击此处跳转](https://roboflow.com/ \"请点击此处跳转\")**\n\n### 搜索自己想要的数据集流程\n\n成功利用神秘力量以后并点击连接后会跳转下面的网址,\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/2330ab877e144be58a4264f69457bce1.png)\n\n点击Get Started之后如果你没有之前登录过该网站就会跳转登录和注册的页面\n\n![](https://img-blog.csdnimg.cn/a054b6098518427d8ecb840d46f7cf63.png) 这里有三个登录方式我推荐的是用Google账号也就是第一个选项，点击之后跳转如下界面,\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/83af5512f3b048189a971c3ca94e4bed.png)\n\n  **如果你有Google账户那么就直接登录即可,如果你没有Google账户那么就需要创建一个账号，点击Create account按照操作流程输入即可需要注意的是这里需要一个手机号验证相比于chatgpt的不同这里的Google账户是可以输入中国的手机号的，但是你用中国手机号注册的账户的Google是登陆不了chatgpt的。**\n\n**当注册完账号并登录之后就跳转以下界面**\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/4116ba4cba7a43feaa3ae597e950bf19.png)\n\n在开始之前我们需要创建一个工作组,就是左上角所显示的Workspaces流程如下图所示 \n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/18f99ea2585240a68a9be1f457693ef3.png)\n\n 按照上面操作之后就建立完成了我们的Workspaces\n\n点击下面图示选项\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/725df3223aaf49aa8f065a60aca9db97.png)\n\n跳转如下界面\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/5b62b21f8616430088b28f006e8ff810.png)\n\n 在上图所示位置输入你想要搜索的数据集**(需要注意的是这里需要输入英文的名称不像Github那样你输入中文名字它可以给你对应搜索中文的，)** \n\n这里假如我想搜索mask(口罩数据集)他就会跳出一堆数据集提供给你选择\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/f9b8fb22d99a48b99393ee39f6eed8a8.png)\n\n我们随便选择一个点击进去\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/d39aeb842f7f47f5a3e411b62f1727cd.png) 然后跳出选项框操作流程如下图所示\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/682b82d399054b718f8e0a5c0ca0ac7c.png) 根据你想要的格式进行选择,然后记得选择Zip格式进行下载到本地(下载到你的浏览器默认下载地址)跳出第三个框即代表你下载完成了。图二中的第二个选项是你进行一些在线训练时候所用的code代码,如果有需要我后期也会出教程，除此之外roboflow还有需要强大的功能如下：\n\n1.  数据增强：Roboflow支持多种数据增强的选项，如旋转、翻转、缩放等，可以帮助开发者扩充数据集，提高模型的泛化能力和准确率。\n    \n2.  数据预处理：Roboflow提供了多种数据预处理的选项，如去除背景、裁剪、缩放等，可以帮助开发者更好地优化训练数据，提高模型的准确率。\n    \n3.  数据集管理：Roboflow可以帮助开发者管理数据集，包括上传、下载、删除等操作，方便管理和使用数据集。\n    \n4.  API支持：Roboflow提供了API支持，可以与其他工具和平台进行集成，方便开发者在不同的应用场景中使用数据集。\n    \n\n"},{"title":"报错","tags":["论文","python"],"categories":["YOLO"],"author":"Louaq","excerpt":"\n\n\n一、本文介绍\n------\n\n本文为专栏内读者和我个人在训练**YOLOv8时遇到的各种错误解决方案**，你遇到的问题本文基本上都能够解决。\n\n二、 报错问题 \n--------\n\n\\# 以下为","link":"/posts/article_14","content":"\n\n\n一、本文介绍\n------\n\n本文为专栏内读者和我个人在训练**YOLOv8时遇到的各种错误解决方案**，你遇到的问题本文基本上都能够解决。\n\n二、 报错问题 \n--------\n\n\\# 以下为两个重要库的版本，大家可以对应下载，使用教程我会更新，时间还没来得及大家可以先看视频使用。\n\n> **项目环境：**\n> \n> python == 3.9.7\n> \n> pytorch == 1.12.1\n> \n> timm == 0.9.12\n> \n> mmcv-full == 1.6.2\n\n* * *\n\n### (1)训练过程中loss出现Nan值.\n\n可以尝试关闭AMP混合精度训练，如何关闭amp呢找到如下文件'ultralytics/cfg/default.yaml'，其中有一个参数是\n\namp: False  # (bool) Automatic Mixed Precision (AMP) training, choices=\\[True, False\\], True runs AMP check\n\n我们将其设置为False即可，默认时为True。\n\n.\n\n### (2)多卡训练问题,修改模型以后不能支持多卡训练可以尝试下面的两行命令行操作，两个是不同的操作，是代表不同的版本现尝试第一个不行用第二个\n\n    python -m torch.distributed.run --nproc\\_per\\_node 2 train.py\n\n    python -m torch.distributed.launch --nproc\\_per\\_node 2 train.py\n\n* * *\n\n### (3) 针对运行过程中的一些报错解决\n\n    **1.如果训练的过程中验证报错了(主要是一些形状不匹配的错误这是因为验证集的一些特殊图片导致)**\n\n就是有这种训练第一个epochs完成后开始验证的时候报错，下面的方法基本百分之九十都能够解决。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/e61c95278a244aebbe4ac67f07f90466.png)\n\n    找到ultralytics/models/yolo/detect/train.py的DetectionTrainer class中的build\\_dataset函数中的rect=mode == 'val'改为rect=False\n\n    **2.推理的时候运行detect.py文件报了形状不匹配的错误**\n\n    找到ultralytics/engine/predictor.py找到函数def pre\\_transform(self, im),在LetterBox中的auto改为False\n\n    **3.训练的过程中报错类型不匹配的问题**\n\n    找到'ultralytics/engine/validator.py'文件找到 'class BaseValidator:' 然后在其'\\_\\_call\\_\\_'中\n\n    self.args.half = self.device.type != 'cpu'  # force FP16 val during training的一行代码下面加上self.args.half = False\n\n* * *\n\n### (4) 针对yaml文件中的nc修改\n\n    不用修改，模型会自动根据你数据集的配置文件获取。\n\n    这也是模型打印两次的区别，第一次打印出来的就是你选择模型的yaml文件结构，第二次打印的就是替换了你数据集的yaml文件，模型使用的是第二种。\n\n* * *\n\n### (5) 针对环境的问题\n\n    环境的问题我实在解决不过来，所以大家可以自行在网上搜索解决方案。  \n\n* * *\n\n### (6) 训练过程中不打印GFLOpS\n\n计算的GFLOPs计算异常不打印，所以需要额外修改一处， 我们找到如下文件'ultralytics/utils/torch\\_utils.py'文件内有如下的代码按照如下的图片进行修改，大家看好函数就行，其中红框的640可能和你的不一样， 然后用我给的代码替换掉整个代码即可。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/24068f6039b94ceeb91e98642c00e594.png)\n\n```python\ndef get_flops(model, imgsz=640):\n    \"\"\"Return a YOLO model's FLOPs.\"\"\"\n    try:\n        model = de_parallel(model)\n        p = next(model.parameters())\n        # stride = max(int(model.stride.max()), 32) if hasattr(model, 'stride') else 32  # max stride\n        stride = 640\n        im = torch.empty((1, 3, stride, stride), device=p.device)  # input image in BCHW format\n        flops = thop.profile(deepcopy(model), inputs=[im], verbose=False)[0] / 1E9 * 2 if thop else 0  # stride GFLOPs\n        imgsz = imgsz if isinstance(imgsz, list) else [imgsz, imgsz]  # expand if int/float\n        return flops * imgsz[0] / stride * imgsz[1] / stride  # 640x640 GFLOPs\n    except Exception:\n        return 0\n```\n\n* * *\n\n### (7) mmcv安装的解决方法\n\n有的读者mmcv-full会安装失败是因为自身系统的编译工具有问题，也有可能是环境之间安装的有冲突 推荐大家离线安装的形式,下面的地址中大家可以找找自己的版本,下载到本地进行安装。 https://download.openmmlab.com/mmcv/dist/cu111/torch1.8.0/index.html https://download.openmmlab.com/mmcv/dist/index.html \n\n"},{"title":"测试","tags":[],"categories":[],"author":"Louaq","excerpt":"\n# 测试\n\n![https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/2719a5e7-d7bb-417b-a338-a22d881f1d72","link":"/posts/article_15","content":"\n# 测试\n\n![https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/2719a5e7-d7bb-417b-a338-a22d881f1d72.jpg](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/2719a5e7-d7bb-417b-a338-a22d881f1d72.jpg)\n\n\n"},{"title":"恒源云","tags":[],"categories":[],"author":"Louaq","excerpt":"\n # 恒源云\n\n为当涉及到深度学习的训练任务时，GPU的计算能力是不可或缺的。相对于传统的中央处理器（CPU），图形处理器（GPU）具有更强大的并行计算能力，能够显著加速深度学习模型的训练过程。深度","link":"/posts/article_2","content":"\n # 恒源云\n\n为当涉及到深度学习的训练任务时，GPU的计算能力是不可或缺的。相对于传统的中央处理器（CPU），图形处理器（GPU）具有更强大的并行计算能力，能够显著加速深度学习模型的训练过程。深度学习算法通常涉及大量的矩阵运算和张量操作，而GPU的并行计算架构使得它们能够高效地执行这些计算，从而加速模型训练的速度。\n\n恒源云是一个经济高效的云计算平台，您可以通过恒源云的控制台或者命令行界面来管理实例、上传和下载数据、执行训练任务等。恒源云还提供了高度可定制的实例规格，您可以根据自己的需求选择适合的实例类型和配置，以最大程度地优化性能和成本。\n\n另一个恒源云的优势是其**经济实惠的价格**。相对于购买和维护专门的GPU设备，利用恒源云进行云端模型训练可以大大节省成本。恒源云提供了多种付费模式，包括按需付费和预付费套餐，使您能够根据自己的预算和需求进行灵活选择。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/3d1ee5ffbd434e55b5d844b892b57423.png)\n\n上传数据集\n-----\n\n在恒源云中我们需要通过终端来上传数据集文件，当在本地处理好了数据集文件以后，我们将其解压缩成zip文件的格式当然tar压缩包等格式的都可以。 \n\n这里推荐大家用OSS命令上传数据集,可以支持大规模的数据上传。\n\n在利用OSS进行上传之前我们需要下载一个文件，下载方式如下。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/2e6a644b805c4ab491ffc9a06b4d0acc.png)\n\n完成之后，我们点击下载好的文件，会弹出命令行。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/c9daf9ea8fef41edb01f7dfd6a420e28.png)\n\n在这里我们可以输入指令,我们先来输入version来检验下我们是否安装成功。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/490f8eb1388b4ccd9d79a464de20960c.png)\n\n当我们安装成功之后，我们先远程登录我们的账号和密码，输入Login\n\n```undefined\nlogin\n```\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/8c9b62f6cf8a47789acca1a7079fb06f.png)\n\n 当我们登录成功之后,我们就远程登录了我们的恒源云账号和密码,我们就可以在我们的账号下面建立存储我们数据的文件了。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/059efdcfd75548f9912456c145124dc7.png) 按照下图操作即可。![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/bf26c45c981f4f3a824245a7cf03a354.png)\n\n当我们上传好一个文件之后,该文件就保存到我们的系统内了,我们可以随时在该终端页面下载该数据到我们后面步骤中创建的任何实例当中，利用如下命令\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0910b808ea634c318316c93eaf7e694a.png) (PS:最后一步需要我们经过下面的'利用云端训练YOLOv8模型'之后才可以进行，在我们创建完实例之后进行的操作步骤)\n\n利用云端训练YOLOv8模型\n--------------\n\n首先进入恒源云的官方网站\n\n[恒源云官方网站](https://www.gpushare.com/ \"恒源云官方网站\")\n\n然后进行注册和登录操作此步骤省略\n\n 当我们注册和登录之后会进到控制台界面,然后点击创建实例进入到如下界面。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0f55027db1194e14abc71c9bcf5bfa0d.png)\n\n在其中根据你的需求选择你的GPU型号,\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/dc99d3b2734e493aa405e8b80dc69dae.png) 之后在同页面的最下面有一个实例镜像，可以在其中的下拉滚动条中选择你需要的PyTorch、TensorFlow或者其它框架的版本\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/8f54226cfeaa498594522102f26048a5.png)\n\n然后之后我们创建实例即可。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/5f764bb992ca476689322218d7c84146.png) 首先开始时需要创建一会,然后才可以进行操作，等待一会创建成功后就会变成如下图的状态情况。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/da777e8327cd43cd8c9962b2d1307e17.png) 我们按照图片的操作点击其中的\"JupyterLab\" 然后会弹出新的网页如下图。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/aa99c7769ca1457da7300c73277351e4.png)\n\n在其中hy-tmp是一个存放我们文件的文件夹,我们点击进去点击图片上的上传本地文件操作即可上传你的模型文件。终端就是一个输入命令的地方，**我们点击终端命令，如下图所示。**\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/4fc533b4a44645a19c95b04380f76afb.png)\n\n我们初始的时候是在系统的根目录下面,我们进行模型训练等操作进入hy-tmp目录也就是你上传文件的目录下面。\n\n我们利用cd 命令进入hy-tmp目录\n\n```bash\ncd hy-tmp\n```\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/bec73eccac7a47dbbd6af209ab67edee.png)\n\n进入其中以后，上传我们的文件。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/8e52743f93b343bab86ff0e68b67c5dd.png) 可以看到我把YOLOv8的官方下载的压缩包上传了进去，其为zip格式的压缩包。\n\n此时在命令行输入命令解压缩该文件\n\n输入unzip 文件名.zip解压文件\n\n```python\nunzip 文件名.zip\n```\n\ncd到该文件目录\n\n```bash\ncd 文件名\n```\n\n输入ll 看文件目录下的结构 \n\n```undefined\nll\n```\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a7ed129b0bf440e3b596b1962f61336a.png)\n\n这里我们演示的是利用YOLOv8进行目标检测时候的训练流程进行演示,我们进入ultralytics\\\\cfg文件目录利用cd进入\n\n```bash\ncd ultralytics\\cfg\n```\n\n同理我们输入ll看该文件下的目录结构\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/1835b338dc514718ae30ecd9bf9229bc.png)\n\n可以看到其中有一个default.yaml文件,该文件就是我们进行训练模型的文件,我们可以在左侧的目录下看该文件的代码。 \n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/536cbabd66be4ad9aa74729c5ff09b7c.png)\n\n当然其中的配置,我就不在这里描述了,如果有需要可以看我的YOLOv8详细训练教程里面有具体的配置以及教程。当我们配置好了数据集以及选择的模型之后就可以在官方的模型基础上进行训练了de。 \n\n此时我们需要退到ultralytics-main的目录下面执行下面的文件就可以进行训练了。\n\n```python\nyolo task=detect mode=train model=datasets/yolo8n.yaml  data=datasets/data.yaml epochs=100 batch=64 device=0 single_cls=True pretrained=yolov8n.pt\n```\n\nPS：在我们的系统中python解释器已经默认帮我们配置好了,如果你想要执行一个py格式文件，我们只需要输入python  文件名.py文件即可\n\n```python\npython 文件名.py\n```\n\n到此本教程就结束,希望对你有所帮助。大家如有任何问题可以在评论区进行提问。 \n\n"},{"title":"YOLOv8目录结构","tags":[],"categories":[],"author":"Louaq","excerpt":"\n\n\n # 目录结构\n\n一、本文介绍\n------\n\nHello，大家好这次给大家带来的不是改进，**是整个YOLOv8项目的分析**，**整个系列大概会更新7-10篇左右的文章**，从项目的目录到每","link":"/posts/article_3","content":"\n\n\n # 目录结构\n\n一、本文介绍\n------\n\nHello，大家好这次给大家带来的不是改进，**是整个YOLOv8项目的分析**，**整个系列大概会更新7-10篇左右的文章**，从项目的目录到每一个功能代码的都会进行详细的讲解，同时YOLOv8改进系列也突破了三十篇文章，最后预计本专栏持续更新会在年底更新上百篇的改进教程， 所以大家如果没有订阅专栏可以提前订阅以下。下面开始进行YOLOv8逐行解析的第一篇——**项目目录构造分析**\n\n二、项目目录构造分析\n----------\n\n开始之前先把源代码的地址分析给大家->\n\n> **官方代码地址：**[YOLO仓库下载地址](https://github.com/ultralytics/ultralytics \"YOLO仓库下载地址\")\n\n下面的图片是我们从仓库上下载整个打开之后的图片，左边的部分是文件，右面呢就是展示窗口。 \n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/df2b0cc18959403993dcc63056305aa2.png)\n\n**下面的是文件部分的清晰截图->**\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/b56eceb48b814fb89370fa557e6da0b6.png)\n\n**下面我们来逐个分析左边的文件各个都是什么作用->**\n\n* * *\n\n### **2.1 .github**\n\n**该目录包含以下内容：**\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/b6f4dfdb894a451289b00660e468521e.png)\n\n> ISSUE\\_TEMPLATE：提供不同类型的问题报告模板，包括 bug-report.yml、config.yml、feature-request.yml和 question.yml。这些模板帮助用户以结构化的方式报告错误、提出功能请求或提问。  \n\n> workflows：包含多个工作流文件，如 ci.yml（持续集成）、cla.yml（贡献者许可协议）、codeql.yml（代码质量检查）、docker.yml（Docker配置）、greetings.yml（自动问候新贡献者）、links.yml、publish.yml（自动发布）、stale.yml（处理陈旧问题）。\n\ndependabot.yml（自动依赖更新）\n\n这些文件共同支持项目的自动化管理，包括代码质量保证、持续集成和部署、社区互动和依赖项维护。\n\n* * *\n\n### 2.2  docker\n\n**该目录包含以下内容：**\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/1b9c076093ff49e6abf177bca6b1ee6b.png)\n\ndocker 目录包含多个 Dockerfile，每个文件都是为不同环境或平台配置的，例如：\n\n*   Dockerfile: 主要的Docker配置文件，用于构建项目的默认Docker镜像。\n*   Dockerfile-arm64: 针对ARM64架构的设备（如某些类型的服务器或高级嵌入式设备）定制的Docker配置。\n*   Dockerfile-conda: 使用Conda包管理器配置环境的Docker配置文件。\n*   Dockerfile-cpu: 为不支持GPU加速的环境配置的Docker配置文件。\n*   Dockerfile-jetson: 专为NVIDIA Jetson平台定制的Docker配置。\n*   Dockerfile-python: 可能是针对纯Python环境的简化Docker配置。\n*   Dockerfile-runner: 可能用于配置持续集成/持续部署（CI/CD）运行环境的Docker配置。\n\n这些配置文件是用来部署用的，用户可以根据自己的需要选择合适的环境来部署和运行项目。\n\n* * *\n\n### 2.3 docs \n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/6895d5eb8af94fa7af4e0d571d845203.png)\n\ndocs目录通常用于存放文档资料，包括多种语言的翻译。例如，此目录下有多个文件夹，每个文件夹代表一种语言（如en代表英语文档）。除此之外，还有几个重要的Python脚本和配置文件给大家说一下：\n\n> build\\_docs.py：一个Python脚本，用于自动化构建和编译文档的过程。  \n> mkdocs.yml：MkDocs配置文件，用于指定文档网站的结构和设置。\n\n以mkdocs\\_es.yml为例，这是用于构建西班牙语文档的MkDocs配置文件。类似的，mkdocs\\_zh.yml用于构建中文文档。所以这些文档其实和我们学习YOLOv8没啥太大的关系，**大家了解以下就可以了**。\n\n* * *\n\n### 2.4 examples\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/8d4825a9f0e84cdb8e7b664860d1d342.png)\n\n在examples文件夹中，大家可以找到不同编程语言和平台的YOLOv8实现示例：\n\nYOLOv8-CPP-Inference：包含C++语言实现的YOLOv8推理示例，内有CMakeLists.txt（用于项目构建的CMake配置文件），inference.cpp和inference.h（推理相关的源代码和头文件），main.cpp（主程序入口）以及README.md（使用说明）。\n\nYOLOv8-ONNXRuntime：提供Python语言与ONNX Runtime结合使用的YOLOv8推理示例，其中main.py是主要的脚本文件，README.md提供了如何使用该示例的指南。\n\nYOLOv8-ONNXRuntime-CPP：与上述ONNX Runtime类似，但是是用C++编写的，包含了相应的CMakeLists.txt，inference.cpp，inference.h和main.cpp文件，以及用于解释如何运行示例的README.md。\n\n每个示例都配有相应的文档，是当我们进行模型部署的时候在不同环境中部署和使用YOLOv8的示例。\n\n* * *\n\n### 2.5 tests\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/f0bdd6022b9043bbaad7e85d9b2c2979.png)\n\ntests目录包含了项目的自动化测试脚本，每个脚本针对项目的不同部分进行测试：\n\nconftest.py：包含测试配置选项或共享的测试助手函数。  \ntest\\_cli.py：用于测试命令行界面（CLI）的功能和行为。  \ntest\\_cuda.py：专门测试项目是否能正确使用NVIDIA的CUDA技术，确保GPU加速功能正常。  \ntest\\_engine.py：测试底层推理引擎，如模型加载和数据处理等。  \ntest\\_integrations.py：测试项目与其他服务或库的集成是否正常工作。  \ntest\\_python.py：用于测试项目的Python API接口是否按预期工作。\n\n这些测试脚本确保大家在改进了文件之后更新或添加的新功能后仍能运行的文件。\n\n* * *\n\n### 2\\. 6 runs \n\n这个文件我们在上面目录构造没有看到是因为，这是我们成功训练了一次模型之后生成的文件，里面保存我们每一次训练之后的各种信息。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/2bb4fc2a037047629dc81e480bf7331b.png)\n\n下面的是训练成功之后的一个完整保存文件:\n\n\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/bcccc7ee5d5144c08788e20d33166cc1.png)\n\n* * *\n\n### 2.6 utlralytics(重点)\n\n上面讲的大部分文件其实对于大部分读者都用不上，这里的**utralytics文件才是重点**，包含了YOLOv8的所有功能都集成在这个文件目录下面，这里我只介绍每一个目录的功能，每一个文件的内部代码我会在接下来的几个博客里面详细的讲到。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0e171e9dca9c48fcb96510e2e6cd8726.png)\n\n#### **2.6.1 assets** \n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/87f3ca08eac94fca888baa2f09e60a54.png)\n\n这个文件下面保存了YOLO历史上可以说最最最经典的两张图片了，这个是大家用来基础推理时候的图片，给大家测试用的。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/e624e36a10ae49b891b8045333ab6f5c.png)\n\n#### 2.6.2 cfg（重点）\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0c08501716d94128a14644311c8accf7.png)\n\n这个文件下面保存了我们的模型配置文件，cfg目录是项目配置的集中地，其中包括：\n\n**datasets文件夹**：包含数据集的配置文件，如数据路径、类别信息等（就是我们训练YOLO模型的时候需要一个数据集，这里面就保存部分数据集的yaml文件，如果我们训练的时候没有指定数据集则会自动下载其中的数据集文件，但是很容易失败！）。  \n**models文件夹**：存放模型配置文件，定义了模型结构和训练参数等，这个是我们改进或者就基础版本的一个yaml文件配置的地方，截图如下:\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/c7117aced6e44708a1df19915508c59d.png)\n\nmodels文件夹中的每个.yaml文件代表了不同的YOLOv8模型配置，具体包括：\n\n**yolov8.yaml:**   这是YOLOv8模型的标准配置文件，定义了模型的基础架构和参数。  \n**yolov8-cls.yaml:** 配置文件调整了YOLOv8模型，专门用于图像分类任务。  \n**yolov8-ghost.yaml:** 应用Ghost模块的YOLOv8变体，旨在提高计算效率。  \n**yolov8-ghost-p2.yaml 和 yolov8-ghost-p6.yaml:** 这些文件是针对特定大小输入的Ghost模型变体配置。  \n**yolov8-p2.yaml和 yolov8-p6.yaml:** 针对不同处理级别（例如不同的输入分辨率或模型深度）的YOLOv8模型配置。  \n**yolov8-pose.yaml:** 为姿态估计任务定制的YOLOv8模型配置。  \n**yolov8-pose-p6.yaml:** 针对更大的输入分辨率或更复杂的模型架构姿态估计任务。  \n**yolov8-rtdetr.yaml:** 可能表示实时检测和跟踪的YOLOv8模型变体。  \n**yolov8-seg.yaml 和 yolov8-seg-p6.yaml:** 这些是为语义分割任务定制的YOLOv8模型配置。\n\n这些配置文件是模型训练和部署的核心，同时大家如果进行改进也是修改其中的对应文件来优化 网络结构。\n\n**trackers文件夹**：用于追踪算法的配置。  \n**\\_\\_init\\_\\_.py文件**：表明\\`cfg\\`是一个Python包。  \n**default.yaml**：项目的默认配置文件，包含了被多个模块共享的通用配置项。\n\n这个文件就是配置训练的时候进行用的然后一些任务选择部分\n\n#### 2.6.3 data\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/dc76024ccabc4de3982b1e7f37620708.png)\n\n在data/scripts文件夹中，包括了一系列脚本和Python文件：\n\n\\- download\\_weights.sh: 用来下载预训练权重的脚本。  \n\\- get\\_coco.sh, get\\_coco128.sh, get\\_imagenet.sh: 用于下载COCO数据集完整版、128张图片版以及ImageNet数据集的脚本。  \n    \n**在data文件夹中，包括：**\n\n**annotator.py:** 用于数据注释的工具。  \n**augment.py:** 数据增强相关的函数或工具。  \n**base.py, build.py, converter.py:** 包含数据处理的基础类或函数、构建数据集的脚本以及数据格式转换工具。  \n**dataset.py:** 数据集加载和处理的相关功能。  \n**loaders.py:** 定义加载数据的方法。  \n**utils.py:** 各种数据处理相关的通用工具函数。\n\n#### 2.6.4  engine\n\nengine文件夹包含与模型训练、评估和推理有关的核心代码：\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/7800024fbeae41cc8f009745c4101fc5.png)\n\n**exporter.py:** 用于将训练好的模型导出到其他格式，例如ONNX或TensorRT。  \n**model.py:** 包含模型定义，还包括模型初始化和加载的方法。  \n**predictor.py:** 包含推理和预测的逻辑，如加载模型并对输入数据进行预测。  \n**results.py:** 用于存储和处理模型输出的结果。  \n**trainer.py:** 包含模型训练过程的逻辑。  \n**tuner.py:** 用于模型超参数调优。  \n**validator.py:** 包含模型验证的逻辑，如在验证集上评估模型性能。\n\n#### 2.6.5 hub\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/5942a5cb41504caaa71e9d1017c6b0bb.png)\n\nhub文件夹通常用于处理与平台或服务集成相关的操作，包括：\n\n**auth.py:** 处理认证流程，如API密钥验证或OAuth流程。  \n**session.py:** 管理会话，包括创建和维护持久会话。  \n**utils.py:** 包含一些通用工具函数，可能用于支持认证和会话管理功能。\n\n#### 2.6.6 models(重点)\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/e29f4a22943e4ce7ad1cc84b247d99e3.png)\n\n这个目录下面是YOLO仓库包含的一些模型的方法实现，我们这里之说YOLO的，同时这里只是简单介绍，后面的博客针对于其中的任意一个都会进行单独的讲解。\n\n这个models/yolo目录中包含了YOLO模型的不同任务特定实现：\n\n**classify:** 这个目录可能包含用于图像分类的YOLO模型。  \n**detect:** 包含用于物体检测的YOLO模型。  \n**pose:** 包含用于姿态估计任务的YOLO模型。  \n**segment:** 包含用于图像分割的YOLO模型，\n\n#### 2.6.7 nn(重点)\n\n这个文件目录下的所有文件，就是定义我们模型中的一些组成构建，之后我们进行改进和优化，增加其它结构的时候都要在对应的文件下面进行改动。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/22a54c754e424b2d808cc532e7a23c47.png)\n\n**modules文件夹:**  \n   **\\_\\_init\\_\\_.py:** 表明此目录是Python包。  \n   **block.py:** 包含定义神经网络中的基础块，如残差块或瓶颈块。  \n   **conv.py:** 包含卷积层相关的实现。  \n   **head.py:** 定义网络的头部，用于预测。  \n   **transformer.py:** 包含Transformer模型相关的实现。  \n   **utils.py:** 提供构建神经网络时可能用到的辅助函数。\n\n**\\_\\_init\\_\\_.py:** 同样标记这个目录为Python包。\n\n**autobackend.py:** 用于自动选择最优的计算后端。\n\n**tasks.py**: 定义了使用神经网络完成的不同任务的流程，例如分类、检测或分割，所有的流程基本上都定义在这里，定义模型前向传播都在这里。\n\n#### 2.6.8 solutions\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/b4b0a147cae94419b8e094c2d0c2c6e9.png)\n\n**\\_\\_init\\_\\_.py:** 标识这是一个Python包。  \n**ai\\_gym.py:** 与强化学习相关，例如在OpenAI Gym环境中训练模型的代码。  \n**heatmap.py:** 用于生成和处理热图数据，这在物体检测和事件定位中很常见。  \n**object\\_counter.py:** 用于物体计数的脚本，包含从图像中检测和计数实例的逻辑。\n\n#### 2.6.9 trackers\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/db2b42c30d7c4db29f9fc02b3c114c47.png)\n\n**trackers**文件夹包含了实现目标跟踪功能的脚本和模块：\n\n**\\_\\_init\\_\\_.py:** 指示该文件夹是一个Python包。  \n**basetrack.py:** 包含跟踪器的基础类或方法。  \n**bot\\_sort.py:** 实现了SORT算法（Simple Online and Realtime Tracking）的版本。  \n**byte\\_tracker.py:** 是一个基于深度学习的跟踪器，使用字节为单位跟踪目标。  \n**track.py:** 包含跟踪单个或多个目标的具体逻辑。  \n**README.md:** 提供该目录内容和用法的说明。\n\n#### 2.6.10 utils\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/7267573c5a5940a796e373dbdbcda494.png)\n\n这个utils目录包含了多个Python脚本，每个脚本都有特定的功能：\n\n**callbacks.py:** 包含在训练过程中被调用的回调函数。  \n**autobatch.py:** 用于实现批处理优化，以提高训练或推理的效率。  \n**benchmarks.py:** 包含性能基准测试相关的函数。  \n**checks.py**: 用于项目中的各种检查，如参数验证或环境检查。  \n**dist.py:** 涉及分布式计算相关的工具。  \n**downloads.py:** 包含下载数据或模型等资源的脚本。  \n**errors.py:** 定义错误处理相关的类和函数。  \n**files.py:** 包含文件操作相关的工具函数。  \n**instance.py:** 包含实例化对象或模型的工具。  \n**loss.py:** 定义损失函数。  \n**metrics.py:** 包含评估模型性能的指标计算函数。  \n**ops.py:** 包含自定义操作，如特殊的数学运算或数据转换。  \n**patches.py:** 用于实现修改或补丁应用的工具。  \n**plotting.py:** 包含数据可视化相关的绘图工具。  \n**tal.py:** 一些损失函数的功能应用  \n**torch\\_utils.py:** 提供PyTorch相关的工具和辅助函数，包括GFLOPs的计算。  \n**triton.py:** 可能与NVIDIA Triton Inference Server集成相关。  \n**tuner.py:** 包含模型或算法调优相关的工具。\n\n**到这里重点的ultralytics文件目录下的所有功能都介绍完毕了，这里只是简单的介绍，后面的博客会详细的介绍一些重要的功能。**\n\n* * *\n\n### 2.7 同级目录下的文件\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/91ff9a17779d4019ada156f9fca35155.png)\n\n**这个里是项目的根本配置和文档文件：**\n\n**.gitignore:** Git配置文件，指定了Git版本控制要忽略的文件。  \n**.pre-commit-config.yaml:** 预提交钩子的配置文件，用于在提交前自动执行代码质量检查。  \n**CITATION.cff:** 提供了如何引用该项目的格式说明。  \n**CONTRIBUTING.md:** 说明如何为项目贡献代码的指南。  \n**LICENSE:** 包含了项目的许可证信息。  \n**MANIFEST.in:** 列出了在构建和分发Python包时需要包含的文件。  \n**README.md 和 README.zh-CN.md:** 项目的说明文件，分别为英文和中文版本。  \n**requirements.txt:** 列出了项目运行所需的Python依赖。  \n**setup.cfg 和 setup.py:** 包含了设置项目安装和分发的脚本。\n\n"},{"title":"YOLOv8文件分析","tags":[],"categories":[],"author":"Louaq","excerpt":"\n# 文件分析\n\n一、本文介绍\n------\n\n本文给大家带来的是**YOLOv8项目的解读**，之前给大家分析了YOLOv8的项目文件分析，这一篇文章给大家带来的是模型训练从我们的yaml文件定义到","link":"/posts/article_4","content":"\n# 文件分析\n\n一、本文介绍\n------\n\n本文给大家带来的是**YOLOv8项目的解读**，之前给大家分析了YOLOv8的项目文件分析，这一篇文章给大家带来的是模型训练从我们的yaml文件定义到模型的定义部分的讲解，我们一般只知道如何去训练模型，和配置yaml文件，但是对于yaml文件是如何输入到模型里，模型如何将yaml文件解析出来的确是不知道的，本文的内容接上一篇的代码逐行解析(一) 项目目录分析，本文对于小白来说非常友好，非常推荐大家进行阅读，深度的了解模型的工作原理已经流程，下面我们从yaml文件来讲解。\n\n本文的讲解全部在代码的对应位置进行注释介绍非常详细，**以下为部分内容的截图。**\n\n![591c0efe630e4e51b8059bf9e8b197c6.png](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/591c0efe630e4e51b8059bf9e8b197c6.png)\n\n![af9d4d78be4d448a98554c9265832fb0.png](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/af9d4d78be4d448a98554c9265832fb0.png)\n\n* * *\n\n二、yaml文件的定义\n-----------\n\n我们训练模型的第一步是需要配置yaml文件，我们的讲解第一步也从yaml文件来开始讲解，YOLOv8的yaml文件存放在我们的如下目录内'ultralytics/cfg/models/v8'，在其中我们可以定义各种模型配置的文件组合不同的模块，我们拿最基础的YOLOv8yaml文件来讲解一下。\n\n**注释部分的内容我就不介绍了，我只介绍一下其中有用的部分，我已经在代码中对应的位置注释上了解释，大家可以看这样看起来也直观一些。**\n\n```python\n# Ultralytics YOLO 🚀, AGPL-3.0 license\n# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect\n \n# Parameters\nnc: 80  # 数据集的类别数，我们默认的数据COCO是80类别（YOLOv8提供的权重也是由此数据集训练出来的），有的读者喜欢修改nc此处其实不需要修改，\n        # 模型会自动根据我们数据集的yaml文件获取此处的数量，同时我们8.1版本之前的ultralytics仓库打印两边的网络结构，唯一的区别就是nc的数量不一样（实际运行的是第二遍的网络结构）。\n \nscales:  # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'\n         # 此处的含义大概就是如果我们在训练的指令时候使用model=yolov8.yaml 则对应的是v8n，如果使用model=yolov8s.yaml则对应的是v8s\n         # 当然了大家如果不想使用上面的方式指定模型，我们只需要将下面想要使用的模型移到最前端即可，或者将其余不想用的注释掉都可以。\n \n  # [depth, width, max_channels]\n  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs\n  s: [0.33, 0.50, 1024]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs\n  m: [0.67, 0.75, 768]   # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs\n  l: [1.00, 1.00, 512]   # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs\n  x: [1.00, 1.25, 512]   # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs\n \n# YOLOv8.0n backbone (主干部分的配置)\nbackbone:\n  # [from, repeats, module, args]\n  # 这里需要多介绍一下，from, repeats, module, args\n  # from 此处有三种可能的值分别是 -1、具体的数值、list存放数值。分别含义如下  (1)、-1的含义就是代表此层的输入就是上一层的输出，\n  #                                                                (2)、如果是具体的某个数字比如4那么则代表本层的输入来自于模型的第四层，\n  #                                                                (3)、有的层是list存放两个值也可能是多个值，则代表对应两个值的输出为本层的输入\n  # repeats 这个参数是为了C2f设置的其它的模块都用不到，代表着C2f当中Bottleneck重复的次数，比如当我们的模型用的是l的时候，那么repeats=3那么则代表C2f当中的Bottleneck串行3个。\n  # module 此处则代表模型的名称\n  # args 此处代表输入到对应模块的参数，此处和parse_model函数中的定义方法有关，对于C2f来说传入的参数->第一个参数是上一个模型的输出通道数，第二个参数就是args的第一个参数，然后以此类推。\n  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2\n  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4\n  - [-1, 3, C2f, [128, True]]\n  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8\n  - [-1, 6, C2f, [256, True]]\n  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16\n  - [-1, 6, C2f, [512, True]]\n  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32\n  - [-1, 3, C2f, [1024, True]]\n  - [-1, 1, SPPF, [1024, 5]]  # 9\n \n# YOLOv8.0n head\nhead:\n  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4\n  - [-1, 3, C2f, [512]]  # 12\n \n  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3\n  - [-1, 3, C2f, [256]]  # 15 (P3/8-small)\n \n  - [-1, 1, Conv, [256, 3, 2]]\n  - [[-1, 12], 1, Concat, [1]]  # cat head P4\n  - [-1, 3, C2f, [512]]  # 18 (P4/16-medium)\n \n  - [-1, 1, Conv, [512, 3, 2]]\n  - [[-1, 9], 1, Concat, [1]]  # cat head P5\n  - [-1, 3, C2f, [1024]]  # 21 (P5/32-large)\n \n  - [[15, 18, 21], 1, Detect, [nc]]  # Detect(P3, P4, P5)\n```\n\n其中的Conv和C2f的结构我就不过多解释了，网上教程已经很多了，其中详细的结构在下图中都能够看到。\n\n![de838aaf62ff43df9cf3a6786cb1ec8f.png](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/de838aaf62ff43df9cf3a6786cb1ec8f.png)\n\n* * *\n\n三、yaml文件的输入 \n------------\n\n上面我们解释了yaml文件中的参数含义，然后提供了一个结构图（其中能够获取到每个模块的详细结构，该结构图来源于官方）。然后我们下一步介绍当定义好了一个ymal文件其是如何传入到模型的内部的，模型的开始在哪里。\n\n### 3.1 模型的定义\n\n我们通过命令行的命令或者创建py文件运行模型之后，模型最开始的工作是模型的定义操作。模型存放于文件'ultralytics/engine/model.py'内部，首先需要通过'\\_\\_init\\_\\_'来定义模型的一些变量。\n\n**此处我将模型的定义部分的代码解释了一下，大家有兴趣的可以和自己的文件对比着看。**\n\n```python\nclass Model(nn.Module):\n    import torch.nn as nn\n \n    class Model(nn.Module):\n        \"\"\"\n        一个统一所有模型API的基类。\n        参数:\n            model (str, Path): 要加载或创建的模型文件的路径。\n            task (Any, 可选): YOLO模型的任务类型。默认为None。\n        属性:\n            predictor (Any): 预测器对象。\n            model (Any): 模型对象。\n            trainer (Any): 训练器对象。\n            task (str): 模型任务类型。\n            ckpt (Any): 如果从*.pt文件加载的模型，则为检查点对象。\n            cfg (str): 如果从*.yaml文件加载的模型，则为模型配置。\n            ckpt_path (str): 检查点文件路径。\n            overrides (dict): 训练器对象的覆盖。\n            metrics (Any): 用于度量的数据。\n        方法:\n            __call__(source=None, stream=False, **kwargs):\n                预测方法的别名。\n            _new(cfg:str, verbose:bool=True) -> None:\n                初始化一个新模型，并从模型定义中推断任务类型。\n            _load(weights:str, task:str='') -> None:\n                初始化一个新模型，并从模型头中推断任务类型。\n            _check_is_pytorch_model() -> None:\n                如果模型不是PyTorch模型，则引发TypeError。\n            reset() -> None:\n                重置模型模块。\n            info(verbose:bool=False) -> None:\n                记录模型信息。\n            fuse() -> None:\n                为了更快的推断，融合模型。\n            predict(source=None, stream=False, **kwargs) -> List[ultralytics.engine.results.Results]:\n                使用YOLO模型进行预测。\n        返回:\n            list(ultralytics.engine.results.Results): 预测结果。\n        \"\"\"\n \n    def __init__(self, model: Union[str, Path] = \"yolov8n.pt\", task=None, verbose=False) -> None:\n        \"\"\"\n        Initializes the YOLO model.\n        Args:\n            model (Union[str, Path], optional): Path or name of the model to load or create. Defaults to 'yolov8n.pt'.\n            task (Any, optional): Task type for the YOLO model. Defaults to None.\n            verbose (bool, optional): Whether to enable verbose mode.\n        \"\"\"\n        \"\"\"\n        此处为上面的解释\n               初始化 YOLO 模型。\n               参数:\n                   model (Union[str, Path], 可选): 要加载或创建的模型的路径或名称。默认为'yolov8n.pt'。\n                   task (Any, 可选): YOLO 模型的任务类型。默认为 None。\n                   verbose (bool, 可选): 是否启用详细模式。\n               \"\"\"\n        super().__init__()\n        \"\"\"此处就是读取我们的yaml文件的地方，callbacks.get_default_callbacks()会将我们的yaml文件进行解析然后将名称返回回来存放在self.callbacks中\"\"\"\n        self.callbacks = callbacks.get_default_callbacks()\n        \"\"\" 下面的部分就是一些模型的参数定义，我大概解释了一下，大家其实也不用太了解，一篇文章也介绍不了太多\"\"\"\n \n        self.predictor = None  # 重用预测器\n        self.model = None  # 模型对象\n        self.trainer = None  # 训练器对象\n        self.ckpt = None  # 如果从*.pt文件加载的检查点对象\n        self.cfg = None  # 如果从*.yaml文件加载的模型配置\n        self.ckpt_path = None  # 检查点文件路径\n        self.overrides = {}  # 训练器对象的覆盖设置\n        self.metrics = None  # 验证/训练指标\n        self.session = None  # HUB 会话\n        self.task = task  # 任务类型\n        self.model_name = model = str(model).strip()  # 去除空格\n \n        # 检查是否为来自 https://hub.ultralytics.com 的 Ultralytics HUB 模型\n        if self.is_hub_model(model):\n            # 从 HUB 获取模型\n            checks.check_requirements(\"hub-sdk>0.0.2\")\n            self.session = self._get_hub_session(model)\n            model = self.session.model_file\n \n        # 检查是否为 Triton 服务器模型\n        elif self.is_triton_model(model):\n            self.model = model\n            self.task = task\n            return\n \n        # 加载或创建新的 YOLO 模型\n        model = checks.check_model_file_from_stem(model)  # 添加后缀，例如 yolov8n -> yolov8n.pt\n        \"\"\" 此处比较重要,如果我们没有指定模型的权重.pt那么模型会根据yaml文件创建一个新的模型，如果指定了权重那么模型这回加载pt文件中的模型\"\"\"\n        if Path(model).suffix in (\".yaml\", \".yml\"):\n            self._new(model, task=task, verbose=verbose)\n        else:\n            self._load(model, task=task)\n \n        self.model_name = model # 返回的模型则保存在self.model_name中\n```\n\n* * *\n\n### 3.2 模型的训练 \n\n我们上面讲完了模型的定义，然后模型就会根据你指定的参数来进行调用对应的函数，比如我这里指定的是detect，和train，如下图所示，然后模型就会根据指定的参数进行对应任务的训练。\n\n**图片来源于文件'ultralytics/cfg/default.yaml' 截图。**\n\n![ba833c58a10243b7b3a30fdc9c137c3e.png](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/ba833c58a10243b7b3a30fdc9c137c3e.png)\n\n此处执行的是ultralytics/engine/model.py'文件中class Model(nn.Module):类别的def train(self, trainer=None, \\*\\*kwargs):函数，具体的解释我已经在代码中标记了。\n\n```python\n        def train(self, trainer=None, **kwargs):\n        \"\"\"\n        在给定的数据集上训练模型。\n        参数:\n            trainer (BaseTrainer, 可选): 自定义的训练器。\n            **kwargs (Any): 表示训练配置的任意数量的参数。\n        \"\"\"\n        self._check_is_pytorch_model()  # 检查模型是否为 PyTorch 模型\n        if hasattr(self.session, \"model\") and self.session.model.id:  # Ultralytics HUB session with loaded model\n            if any(kwargs):\n                LOGGER.warning(\"WARNING ⚠️ 使用 HUB 训练参数，忽略本地训练参数。\")\n            kwargs = self.session.train_args  # 覆盖 kwargs\n \n        checks.check_pip_update_available()  # 检查 pip 是否有更新\n \n        overrides = yaml_load(checks.check_yaml(kwargs[\"cfg\"])) if kwargs.get(\"cfg\") else self.overrides\n        custom = {\"data\": DEFAULT_CFG_DICT[\"data\"] or TASK2DATA[self.task]}  # 方法的默认设置\n        args = {**overrides, **custom, **kwargs, \"mode\": \"train\"}  # 最高优先级的参数在右侧\n        if args.get(\"resume\"):\n            args[\"resume\"] = self.ckpt_path\n \n        # 实例化或加载训练器\n        \"\"\" 此处将一些参数加载到模型的内部\"\"\"\n        self.trainer = (trainer or self._smart_load(\"trainer\"))(overrides=args, _callbacks=self.callbacks)\n \n        if not args.get(\"resume\"):  # 仅在不续训的时候手动设置模型\n            # 获取模型并设置训练器\n            \"\"\"\n            此处比较重要,为开始定义我们的对应任务的模型了比如我这里task设置的为Detect,那么此处会实例化DetectModel模型。\n            模型存放在ultralytics/nn/tasks.py内（就是我们修改模型时候的用到的那个task.py文件）\n            此处就会跳转到'ultralytics/nn/tasks.py'文化内的class DetectionModel(BaseModel):类中进行初始化和模型的定义工作\n            \"\"\"\n            self.trainer.model = self.trainer.get_model(weights=self.model if self.ckpt else None, cfg=self.model.yaml)\n            self.model = self.trainer.model\n \n            if SETTINGS[\"hub\"] is True and not self.session:\n                # 如果开启了 HUB 并且没有 HUB 会话\n                try:\n                    # 创建一个 HUB 中的模型\n                    self.session = self._get_hub_session(self.model_name)\n                    if self.session:\n                        self.session.create_model(args)\n                        # 检查模型是否创建成功\n                        if not getattr(self.session.model, \"id\", None):\n                            self.session = None\n                except (PermissionError, ModuleNotFoundError):\n                    # 忽略 PermissionError 和 ModuleNotFoundError，表示 hub-sdk 未安装\n                    pass\n \n        # 将可选的 HUB 会话附加到训练器\n        self.trainer.hub_session = self.session\n \n        # 进行模型训练\n        self.trainer.train()\n \n        # 训练结束后更新模型和配置信息\n        if RANK in (-1, 0):\n            ckpt = self.trainer.best if self.trainer.best.exists() else self.trainer.last\n            self.model, _ = attempt_load_one_weight(ckpt)\n            self.overrides = self.model.args\n            self.metrics = getattr(self.trainer.validator, \"metrics\", None)  # TODO: DDP 模式下没有返回指标\n        return self.metrics\n```\n\n* * *\n\n### 3.3 模型的网络结构打印\n\n第三步比较重要的就是来到了'ultralytics/nn/tasks.py'（就是我们改进模型时候的那个文件）文化内的class DetectionModel(BaseModel):类中进行初始化和模型的定义工作。\n\n这里涉及到了模型的定义和校验工作（在模型的正式开始训练之前检测模型是否能够运行的工作！）。 \n\n```python\nclass DetectionModel(BaseModel):\n    \"\"\"YOLOv8 目标检测模型。\"\"\"\n \n    def __init__(self, cfg=\"yolov8n.yaml\", ch=3, nc=None, verbose=True):  # model, input channels, number of classes\n        \"\"\"使用给定的配置和参数初始化 YOLOv8 目标检测模型。\"\"\"\n        super().__init__()\n        self.yaml = cfg if isinstance(cfg, dict) else yaml_model_load(cfg)  # cfg 字典\n \n        # 定义模型\n        ch = self.yaml[\"ch\"] = self.yaml.get(\"ch\", ch)  # 输入通道数\n        if nc and nc != self.yaml[\"nc\"]:\n            LOGGER.info(f\"覆盖 model.yaml nc={self.yaml['nc']} 为 nc={nc}\")\n            self.yaml[\"nc\"] = nc  # 覆盖 YAML 中的值\n        \"\"\" 此处最为重要，涉及到了我们修改模型的配置的那个函数parse_model,\n            这里返回了我们的每一个模块的定义，也就是self.model保存了我们的ymal文件所有模块的实例化模型\n            self.save保存列表 | 也就是除了from部分为-1的部分比如from为4那么就将第四层的索引保存这里留着后面备用，\n        \"\"\"\n        self.model, self.save = parse_model(deepcopy(self.yaml), ch=ch, verbose=verbose)  # 模型，保存列表\n        self.names = {i: f\"{i}\" for i in range(self.yaml[\"nc\"])}  # 默认名称字典\n        self.inplace = self.yaml.get(\"inplace\", True)\n \n        # 构建步长\n        m = self.model[-1]  # Detect()\n        if isinstance(m, (Detect, Segment, Pose, Detect_AFPN4, Detect_AFPN3, Detect_ASFF, Detect_FRM, Detect_dyhead,\n                          CLLAHead, Detect_dyhead3, Detect_DySnakeConv, Segment_DySnakeConv,\n                          Segment_DBB, Detect_DBB, Pose_DBB, OBB, Detect_FASFF)):\n            s = 640  # 2x 最小步长\n            m.inplace = self.inplace\n            forward = lambda x: self.forward(x)[0] if isinstance(m, (Segment, Segment_DySnakeConv, Pose, Pose_DBB, Segment_DBB, OBB)) else self.forward(x)\n            try:\n                m.stride = torch.tensor([s / x.shape[-2] for x in forward(torch.zeros(1, ch, s, s))])  # 在 CPU 上进行前向传播\n            except RuntimeError:\n                try:\n                    self.model.to(torch.device('cuda'))\n                    m.stride = torch.tensor([s / x.shape[-2] for x in forward(\n                        torch.zeros(1, ch, s, s).to(torch.device('cuda')))])  # 在 CUDA 上进行前向传播\n                except RuntimeError as error:\n                    raise error\n            self.stride = m.stride\n            m.bias_init()  # 仅运行一次\n        else:\n            self.stride = torch.Tensor([32])  # 默认步长，例如 RTDETR\n \n        # 初始化权重和偏置\n        initialize_weights(self)\n        if verbose: # 此处为获取模型参数量和打印的地方。\n            self.info()\n            LOGGER.info(\"\")\n```\n\n\n\n* * *\n\n### 3.4 parse\\_model的解析\n\n这里涉及到yaml文件中模块的定义和，通道数放缩的地方，此处大家可以仔细看看比较重要涉及到模块的改动。\n\n```python\ndef parse_model(d, ch, verbose=True):  # model_dict, input_channels(3)\n    \"\"\"解析 YOLO 模型.yaml 字典为 PyTorch 模型。\"\"\"\n    import ast\n \n    # 参数设置\n    max_channels = float(\"inf\") # 设置一个最大的通道数inf,防止后面的通道数有的超出了范围，没什么作用其实。\n    \"\"\"下面一行代码比较重要，为获取我们yaml文件中的参数,nc=类别数（前面解释过了） act=激活函数， scales=模型的大小\"\"\"\n    nc, act, scales = (d.get(x) for x in (\"nc\", \"activation\", \"scales\"))\n    \"\"\"此处为获取模型的通道数放缩比例假如  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs\"\"\"\n    \"\"\"那么此处对应的就是 0.33 , 0.25, 1024\"\"\"\n    depth, width, kpt_shape = (d.get(x, 1.0) for x in (\"depth_multiple\", \"width_multiple\", \"kpt_shape\"))\n    \"\"\"下面这个判断主要的功能就是我们指定yaml文件的时候如果不指定n或者其它模型尺度则默认用n然后提出一个警告，细心的读者应该会遇到过这个警告，群里也有人问过\"\"\"\n    if scales:\n        scale = d.get(\"scale\")\n        if not scale:\n            scale = tuple(scales.keys())[0]\n            LOGGER.warning(f\"WARNING ⚠️ 没有传递模型比例。假定 scale='{scale}'。\")\n        depth, width, max_channels = scales[scale]\n \n    if act:\n        Conv.default_act = eval(act)  # 重新定义默认激活函数，例如 Conv.default_act = nn.SiLU()\n        if verbose:\n            LOGGER.info(f\"{colorstr('activation:')} {act}\")  # 打印\n \n    if verbose:\n        LOGGER.info(f\"\\n{'':>3}{'from':>20}{'n':>3}{'params':>10}  {'module':<45}{'arguments':<30}\")\n    ch = [ch] # 存放第一个输入的通道数,这个ch后面会存放所有层的通道数，第一层为通道数是ch=3也就是对应我们一张图片的RGB图片的三基色三个通道，分别对应红绿蓝！\n    layers, save, c2 = [], [], ch[-1]  # 提前定义一些之后存放的容器分别为，模型层，保存列表，输出通道数\n    \"\"\"下面开始正式解析模型的yaml文件然后进行定义的操作用for训练便利yaml文件\"\"\"\n    for i, (f, n, m, args) in enumerate(d[\"backbone\"] + d[\"head\"]):  # from, number, module, args\n        m = getattr(torch.nn, m[3:]) if \"nn.\" in m else globals()[m]  # 获取模块\n        for j, a in enumerate(args):\n            if isinstance(a, str):\n                with contextlib.suppress(ValueError):\n                    args[j] = locals()[a] if a in locals() else ast.literal_eval(a)\n        \"\"\" 此处为repeat那个参数的放缩操作,不过多解释了,最小的n是1（就是是说你yaml文件里定义的是3，然后和放缩系数相乘然后和1比那个小取那个）\"\"\"\n        n = n_ = max(round(n * depth), 1) if n > 1 else n\n        \"\"\"下面是一些具体模块的定义操作了\"\"\"\n        if m in (Classify, Conv, ConvTranspose, GhostConv, Bottleneck, GhostBottleneck, SPP, SPPF, DWConv, Focus,\n                 BottleneckCSP, C1, C2, C2f, C2fAttn, C3, C3TR, C3Ghost, nn.ConvTranspose2d, DWConvTranspose2d, C3x, RepC3):\n            c1, c2 = ch[f], args[0]\n            if c2 != nc:  # 如果 c2 不等于类别数（即 Classify() 输出）\n                \"\"\" 绝大多数情况下都不等，我们放缩通道数，也就是为什么不同大小的模型参数量不一致的地方因为参数量主要由通道数决定，GFLOPs主要有图像的宽和高决定\"\"\"\n                c2 = make_divisible(min(c2, max_channels) * width, 8)\n            if m is C2fAttn:\n                args[1] = make_divisible(min(args[1], max_channels // 2) * width, 8)  # 嵌入通道数\n                args[2] = int(\n                    max(round(min(args[2], max_channels // 2 // 32)) * width, 1) if args[2] > 1 else args[2]\n                )  # 头部数量\n            \"\"\"此处需要解释一下，大家需要仔细注意此处\"\"\"\n            \"\"\" 这个args就是传入到我们模型的参数,C1就是上一层的或者指定层的输出的通道数，C2就是本层的输出通道数， *args[1:]就是其它的一些参数比如卷积核步长什么的\"\"\"\n            \"\"\" 此处和注意力机制不同的是，为什么注意力机制不在此处添加因为注意力机制不改变模型的维度，所以一般只需要指定一个输入通道数就行，\n                所以这也是为什么我们在后面定义注意力需要额外添加代码的原因有兴趣的读者可以对比一下\"\"\"\n            args = [c1, c2, *args[1:]]\n            \"\"\" 此处就是涉及的上面求出的实际的n然后插入的参数列表中去，然后准备在最下面进行传参\"\"\"\n            if m in (BottleneckCSP, C1, C2, C2f, C2fAttn, C3, C3TR, C3Ghost, C3x, RepC3):\n                args.insert(2, n)  # 重复次数\n                n = 1\n        \"\"\"这些都是一些具体的模块定义的方法，不多解释了\"\"\"\n        elif m is AIFI:\n            args = [ch[f], *args]\n        elif m in (HGStem, HGBlock):\n            c1, cm, c2 = ch[f], args[0], args[1]\n            args = [c1, cm, c2, *args[2:]]\n            if m is HGBlock:\n                args.insert(4, n)  # 重复次数\n                n = 1\n        elif m is ResNetLayer:\n            c2 = args[1] if args[3] else args[1] * 4\n        elif m is nn.BatchNorm2d:\n            args = [ch[f]]\n        elif m is Concat:\n            c2 = sum(ch[x] for x in f)\n        elif m in (Detect, WorldDetect, Segment, Pose, OBB, ImagePoolingAttn):\n            args.append([ch[x] for x in f])\n            if m is Segment:\n                args[2] = make_divisible(min(args[2], max_channels) * width, 8)\n        elif m is RTDETRDecoder:  # 特殊情况，channels 参数必须在索引 1 中传递\n            args.insert(1, [ch[x] for x in f])\n        else:\n            c2 = ch[f]\n        \"\"\"此处就是模型的正式定义和传参的操作\"\"\"\n        m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)  # 模块\n        t = str(m)[8:-2].replace(\"__main__.\", \"\")  # 模块类型\n        m.np = sum(x.numel() for x in m_.parameters())  # 参数数量\n        m_.i, m_.f, m_.type = i, f, t  # 附加索引，'from' 索引，类型\n        if verbose:\n            LOGGER.info(f\"{i:>3}{str(f):>20}{n_:>3}{m.np:10.0f}  {t:<45}{str(args):<30}\")  # 打印\n        \"\"\"此处就是保存一些索引通道数涉及到from的部分，此处文字很难解释的清楚有兴趣可以自己debug看一下就明白了\"\"\"\n        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # 添加到保存列表\n        layers.append(m_)\n        if i == 0:\n            ch = []\n        ch.append(c2)\n    return nn.Sequential(*layers), sorted(save)\n```\n\n* * *\n\n四、模型的结构打印\n---------\n\n经过上面的分析之后，我们就会打印了模型的结构，图片如下所示，然后到此本篇文章的分析就到这里了，剩下的下一篇文章讲解。\n\n**（需要注意的是上面的讲解整体是按照顺序但是是以递归的形式介绍，比如3.2是3.1当中的某一行代码的功能而不是结束之后才允许的3.2，而是3.1运行的过程中运行了3.2。）**\n\n![d3d9d7580362433ba08904db10be9ea4.png](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/d3d9d7580362433ba08904db10be9ea4.png)\n\n"},{"title":"Hello World","tags":[],"categories":[],"author":"Louaq","excerpt":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hex","link":"/posts/hello-world","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n$$\ni\\hbar\\frac{\\partial}{\\partial t}\\psi=-\\frac{\\hbar^2}{2m}\\nabla^2\\psi+V\\psi\n$$\n\n"},{"title":"说明文档","tags":["人工智能","论文","python"],"categories":["YOLO"],"author":"Louaq","excerpt":"\n\n\n> 这篇文档主要介绍《基于YOLOv8的农田病虫害检测与分析》的代码实现部分，整篇论文的目的主要是改进YOLOv8的网络结构，使其在检测病虫害的精度和实时性上有所提升。接下来，我将介绍如何从零开","link":"/posts/%E8%AF%B4%E6%98%8E%E6%96%87%E6%A1%A3","content":"\n\n\n> 这篇文档主要介绍《基于YOLOv8的农田病虫害检测与分析》的代码实现部分，整篇论文的目的主要是改进YOLOv8的网络结构，使其在检测病虫害的精度和实时性上有所提升。接下来，我将介绍如何从零开始搭建起本项目。\n\n## 安装Python\n\n到python的官方网站：[https://www.python.org/](https://www.python.org/)下载，安装\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-10-42.png)\n\n\n\n安装完成后，在命令行窗口运行：python，查看安装的结果，如下图：\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-14-22.png)\n\n至此，Python安装完成，接下来还需要安装anaconda，这是一个python虚拟环境，特别适合管理python的环境。\n\n## 安装anaconda\n\n到anaconda的官方网站：[https://www.anaconda.com/download/success](https://www.anaconda.com/download/success)下载，并安装：\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-17-10.png)\n\n安装成功后，会在开始菜单出现如下图所示：\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-19-17.png)\n\nanaconda安装完成，接下来安装pycharm，主要用来编写代码。\n\n## 安装Pycharm\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-23-47.png)\n\n学生可以申请教育版\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-24-59.png)\n\n\n\n支持，所有的软件安装完成。\n\n## YOLOv8目录结构介绍\n\n首先介绍整个项目的目录：\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-27-47.png)\n\n\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-28-07.png)\n\n和原来的YOLOv8相比，根目录新增一些训练的脚本和测试的脚本，比如train.py和Detect.py，当然也可以直接通过命令行的方式来实现，两者效果都是一样的。\n\n> **重点是ultralytics/nn目录，所有的改进模块都是在这里进行，在这里我新建了一个Addmodules的目录，里面是改进的各种模块，包括主干网络，颈部网络和检测头的改进。**\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-36-15.png)\n\n需要修改的部分我都已经作了修改，不用再做其他的改动\n\n> **还有一个重要的目录：ultralytics/cfg/models/Add，这里面放的都是yaml文件，其中改进的yaml文件都已经写好，不需要改动。**\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-38-32.png)\n\n以下是一个yaml文件的示例，其它的都是类似的结构，只是参数不同：\n\n## 安装项目的环境（非常重要）\n\n> 环境配置非常重要，我当时配环境换了一周左右的时间，中间经历了各种报错，软件包不兼容的问题和显卡驱动匹配的问题，总之就是不好搞。为了方面复现工作，我已经把anaconda的环境导出为environment.yml，位于项目的根目录里面，创建虚拟环境的时候直接使用就可以\n\n\n\n### anaconda虚拟环境\n\n再anaconda prompt终端输入conda env create -f environment.yml，就可以根据environment.yml文件创建虚拟环境，创建好后，通过conda env list查看环境是否存在，如下图所示就表明创建成功：\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_16-35-14.png)\n\n如果安装的时候出现torch相关的错误，大概率是你的显卡驱动和这里面的torch包版本不匹配，这个问题需要自行修改即可，网上关于这方面的资料很多。\n\n\n\n### 使用虚拟环境\n\n虚拟环境创建完成之后，就可以在pycharm中使用，点击右下角，切换conda环境，选择刚才创建的虚拟环境。如果到了这一步还没有报错的话，恭喜你，已经完成了80%的工作。\n\n运行Detect.py脚本，测试检测效果，如果没有报错，接下来就是训练模型。\n\n\n\n## 训练脚本train.py\n\n找到根目录的train.py文件，注释已经写的很清楚，如下图：\n\n```py\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom ultralytics import YOLO\n\nif __name__ == '__main__':\n    model = YOLO('yolov8-HSFPN.yaml')\n\n    # model.load('yolov8n.pt') # 是否加载预训练权重,科研不建议大家加载否则很难提升精度\n\n    model.train(data=r'D:/Downloads/YOLOv8/datasets/data.yaml',\n                # 如果大家任务是其它的'ultralytics/cfg/default.yaml'找到这里修改task可以改成detect, segment, classify, pose\n                cache=False,\n                imgsz=640,\n                epochs=150,\n                single_cls=False,  # 是否是单类别检测\n                batch=4,\n                close_mosaic=10,\n                workers=0,\n                device='0',\n                optimizer='SGD', # using SGD\n                # resume='runs/train/exp21/weights/last.pt', # 如过想续训就设置last.pt的地址\n                amp=True,  # 如果出现训练损失为Nan可以关闭amp\n                project='runs/train',\n                name='exp',\n                )\n```\n\n\n\nmodel = YOLO('yolov8-HSFPN.yaml')，把里面的yaml文件换成自己的yaml文件，我这里用的是yolov8-HSFPN.yaml，data=r'D:/Downloads/YOLOv8/datasets/data.yaml，同理，换成自己数据集的yaml文件，我这里的数据集是yolo格式。其它的参数可以按照自己的任务自行调整。\n\n\n\n还有一个检测的脚本，Detect.py:\n\n```python\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom ultralytics import YOLO\n\nif __name__ == '__main__':\n    model = YOLO('D:/Downloads/YOLOv8/result/result_8_HSFPN/train/exp/weights/best.pt') # select your model.pt path\n    model.predict(source='D:/Downloads/YOLOv8/ultralytics/assets',\n                  imgsz=640,\n                  project='runs/detect',\n                  name='exp',\n                  save=True,\n                )\n```\n\n同理，把best.pt换成你自己训练好的模型，source里面输入检测图片的路径，运行该脚本就可以开始检测，结果保存在runs/detect目录。\n\n\n\n## 开始训练\n\n准备好数据集，最好是yolo格式的，我的数据集项目里自带了，不需要重新下载：\n\n<img src=\"https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-55-44.png\" style=\"zoom:67%;\" />\n\ndatasets目录里面就是我的数据集：有train，test，valid三个目录，分别存放训练集，测试集和验证集的图像和标签：\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-58-01.png)\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_15-58-32.png)\n\n准备这些之后，运行train.py文件，开始训练。如果报错的话，请自行上网查找，无非就是找不到数据集，某个包的版本不对，或者是GPU用不了，只能用CPU。\n\n## 训练结果\n\n> 训练结果会保存在runs/train目录下，exp1,exp2,exp3的顺序，表示每一次的训练结果。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_16-04-37.png)\n\n上图就是训练完成后目录的结构，weights目录里面就是我们需要的模型：best.pts是效果最好的，最后也是需要这个，last.pt是最后一次的训练结果。\n\n![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/Snipaste_2024-05-23_16-05-47.png)\n\n## 总结\n\n**整个项目的改进工作我已经做好，复现的话只需装好对应的环境，修改train.py的参数，运行train.py就可以开始训练；修改Detect.py的参数，就可以检测。目前项目只针对检测任务，对于分割和分类没有做改进。**\n\n\n\n\n\n## 经验之谈\n\n**（1）以下为两个重要库的版本，必须对应下载，否则会报错**\n\n\n\n> python == 3.9.7\n> pytorch == 1.12.1 \n> timm == 0.9.12  # 此安装包必须要\n> mmcv-full == 1.6.2  # 不安装此包部分关于dyhead的代码运行不了以及Gold-YOLO\n\n\n\n\n\n**（2）mmcv-full会安装失败是因为自身系统的编译工具有问题，也有可能是环境之间安装的有冲突**\n\n    推荐大家离线安装的形式,下面的地址中大家可以找找自己的版本,下载到本地进行安装。\n    https://download.openmmlab.com/mmcv/dist/cu111/torch1.8.0/index.html\n    https://download.openmmlab.com/mmcv/dist/index.html\n\n\n\n**（3）basicsr安装失败原因,通过pip install basicsr 下载如果失败,大家可以去百度搜一下如何换下载镜像源就可以修复**\n\n\n\n### 针对一些报错的解决办法在这里说一下\n\n**(1)训练过程中loss出现Nan值.**\n   可以尝试关闭AMP混合精度训练.\n\n**(2)多卡训练问题,修改模型以后不能支持多卡训练可以尝试下面的两行命令行操作，两个是不同的操作，是代表不同的版本现尝试第一个不行用第二个**\n\n    python -m torch.distributed.run --nproc_per_node 2 train.py\n    python -m torch.distributed.launch --nproc_per_node 2 train.py\n\n**(3) 针对运行过程中的一些报错解决**\n    1.如果训练的过程中验证报错了(主要是一些形状不匹配的错误这是因为验证集的一些特殊图片导致)\n    找到ultralytics/models/yolo/detect/train.py的DetectionTrainer class中的build_dataset函数中的rect=mode == 'val'改为rect=False\n\n```py\n2.推理的时候运行detect.py文件报了形状不匹配的错误\n找到ultralytics/engine/predictor.py找到函数def pre_transform(self, im),在LetterBox中的auto改为False\n\n3.训练的过程中报错类型不匹配的问题\n找到'ultralytics/engine/validator.py'文件找到 'class BaseValidator:' 然后在其'__call__'中\nself.args.half = self.device.type != 'cpu'  # force FP16 val during training的一行代码下面加上self.args.half = False\n```\n\n**(4) 针对yaml文件中的nc修改**\n    不用修改，模型会自动根据你数据集的配置文件获取。\n    这也是模型打印两次的区别，第一次打印出来的就是你选择模型的yaml文件结构，第二次打印的就是替换了你数据集的yaml文件，模型使用的是第二种。\n\n**(5) 针对环境的问题**\n    环境的问题每个人遇见的都不一样，可自行上网查找。\n\n[]: \n\n\n\n"}]
